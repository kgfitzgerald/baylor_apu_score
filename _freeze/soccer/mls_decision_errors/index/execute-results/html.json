{
  "hash": "ce0a4dba8354df46a1314f23ad2adf97",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"MLS - Types of Decision Errors\"\nauthor:\n  - name: Jonathan Lieb\n    email: jonathan_lieb1@baylor.edu\n    affiliation:\n      - id: bay\n        name: Baylor University\ndate: July 30, 2024\nformat:\n  html:\n    grid:\n      margin-width: 350px\ntoc: true\ntoc-location: left\ndescription: Exploring types of decision errors\ncategories:\n  - Decision Errors\neditor_options:\n  chunk_output_type: console\ncallout-icon: false\n---\n\n\n::: {.callout-note collapse=\"true\" title=\"Facilitation notes\" appearance=\"minimal\"}\n+ This module would be suitable for an in-class lab or take-home assignment in an introductory statistics course. \n\n+ The data used to create this module can be downloaded [here](training_facilities.csv)\n\n+ A student template for answers can be downloaded [here](student_template.pdf)\n:::\n\n# Welcome video\n\n<iframe width=\"560\" height=\"315\" src title=\"placeholder\">\n\n</iframe>\n\n# Introduction\n\nIn this module, we will explore the concept of decision errors in hypothesis testing. We will use MLS (Major League Soccer) training facility data to explore Type I and Type II errors. \n\n::: column-margin\nInter Miami CF opened a $40 million training facility in 2020. It sprawls over 25 acres and includes 7 fields. The field is adjacent to their home stadium and can be seen in the background of the image below.\n\n![Chase Stadium](https://upload.wikimedia.org/wikipedia/commons/0/06/Inter_Miami_vs_New_England_Revolution_by_cornfield948_%2820220410085018%29.jpg)\n\nImage Source: <a href=\"https://commons.wikimedia.org/wiki/File:Inter_Miami_vs_New_England_Revolution_by_cornfield948_(20220410085018).jpg\">Cornfield948</a>, <a href=\"https://creativecommons.org/licenses/by-sa/3.0\">CC BY-SA 3.0</a>, via Wikimedia Commons\n\n:::\n\nIn the last decade (2014-2023) more than half of all MLS teams opened new training facilities, and several more clubs have plans to build new facilities soon. These cutting edge facilities are designed to provide players with the best possible environment to train and develop, hopefully translating to better performance on the field.\n\nDo these new, state-of-the-art facilities lead to better performance on the field though? What are some potential consequences of claiming they make a difference when they don't? What are some potential consequences of claiming they don't make a difference when they do?\n\nWe will explore these questions and more as we kick off our exploration of hypothesis testing and decision errors. For our purposes we will consider any team with a training facility that opened in 2014 or later to have a new training facility and any team with a training facility that opened before 2014 to have an old training facility.\n\n::: {.callout-note title=\"Learning Objectives\" appearance=\"minimal\"}\n\nBy the end of this module, you should be able to:\n\n-   Understand the basic concepts of hypothesis testing\n\n-   Define Type I errors\n\n-   Define Type II errors\n\n-   Explain the relationship between Type I and Type II errors\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n# Terms to Know\n\n::: {.callout-important title=\"Soccer Vocab\"}\n\n::: column-margin\n**ON THE RISE**\nNew teams like St. Louis City SC have helped fuel the MLS's growth. St. Louis City SC joined the league in 2023 and has already made a splash.\n\nRatings, attendance, and revenue have all increased dramatically over the last decade. \n\n![St. Louis City SC Logo](https://upload.wikimedia.org/wikipedia/commons/1/18/St._Louis_City_SC_logo.png)\n\nImage Source: <a href=\"https://commons.wikimedia.org/wiki/File:St._Louis_City_SC_logo.png\">IagoQnsi</a>, <a href=\"https://creativecommons.org/licenses/by-sa/4.0\">CC BY-SA 4.0</a>, via Wikimedia Commons\n:::\n\nLet's knock out some soccer terminology before we head into hypothesis testing and decision errors.\n\n-  **MLS**: Major League Soccer. The top professional soccer league in the United States and Canada.\n\n-  **Match**: A soccer game\n\n-  **Points per Match Played**: The average number of points a team earns per match played. In MLS, a team earns 3 points for a win, 1 point for a draw, and 0 points for a loss. This is **not** the amount of goals scored per match.\n\n-  **Training Facility**: A facility where a team practices and trains. These facilities can include fields, gyms, locker rooms, and more. Often these teams share facilities with other teams in the organization.\n\n-  **Table**: The standings in a league. Teams are ranked by the number of points they have earned. The team with the most points is at the top of the table.\n\n:::\n\n# Hypothesis Testing\n\nBefore we can kick off our exploration of decision errors, we need to understand the basics of hypothesis testing.\n\nHypothesis testing, a method used to make decisions about a population parameter based on sample data, is a foundational concept in statistics. The decision is made by comparing the sample data results to a null hypothesis. The null hypothesis is generally a statement that there is no effect or no difference than what is assumed to be the true. An alternative hypothesis is a statement that there is an effect or a difference. A hypothesis test can be done for one sample, two sample, or many samples.\n\n#### Null and Alternative Hypotheses\n\nThe null hypothesis is generally denoted by $H_0$ and the alternative hypothesis is generally denoted by $H_a$. For one sample tests, the null hypothesis is often that the population parameter is equal to a specific value. For example $H_0 : \\mu = \\mu_0$. For multiple sample tests, the null hypothesis often states that there is no difference between the groups. For example $H_0: \\mu_{1} = \\mu_{2} = ... = \\mu_{n}$.\n\n::: column-margin\n**NOTE**: All examples in this section are using the population mean ($\\mu$) as the parameter of interest. Any population parameter can be tested using hypothesis testing. Other common parameters include the population proportion ($p$) and the population variance ($\\sigma^2$). \n\nAn example of a hypothesis test using proportions is shown below:\n\n$$H_0: p = p_0$$\n\n$$H_a: p \\neq p_0$$\n\nWhere $p$ is the population proportion and $p_0$ is the assumed population proportion.\n\n:::\n\nThe alternative hypothesis is a statement that there is an effect or a difference. For a one sample test, the alternative hypothesis is generally that the population parameter is not equal to, greater than, or less than a specific value. For example, $H_a: \\mu \\neq \\mu_0$, $H_a: \\mu > \\mu_0$, or $H_a: \\mu < \\mu_0$. For a multi-sample test, the alternative hypothesis is generally that there is a difference between the groups. $H_a: \\text{ At least one } \\mu_i \\text{ is different}$.\n\n\n#### Setting up a Hypothesis Test\n\nLet's set up a hypothesis test to determine if a new training facility leads to better performance on the field. We will use points per match played as our measure of performance. The null hypothesis is that there is no difference in points per match played between teams with new training facilities and teams without new training facilities. The alternative hypothesis is that the teams with new training facilities have a higher points per match played than teams without new training facilities.\n\n::: column-margin\n**Fun Fact**: The top team in the MLS in 2023, FC Cincinnati, averaged 2.03 points per match. They opened a new training facility in 2019.\n\n<br>\nThe graph below show the points per match played in 2023 for teams with new training facilities and teams without new training facilities.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n:::\n\n$$H_0: \\mu_{new} = \\mu_{old}$$\n\n$$H_a: \\mu_{new} \\ge \\mu_{old}$$\n\nWhere $\\mu_{new}$ is the population mean points per match played for teams with new training facilities and $\\mu_{old}$ is the population mean points per match played for teams without new training facilities.\n\n\n#### Significane Level and Decisions in Hypothesis Testing\nIn hypothesis testing, we make a decision to either reject the null hypothesis or fail to reject the null hypothesis. We make this decision based on the sample data and the **significance level**. The **significance level** is the probability of making a Type I error, which we will expand on soon. The significance level is denoted by **$\\alpha$**. We often use the probability of observing the sample data given that the null hypothesis is true, called the **p-value**, to make our decision. If the **p-value** is less than the significance level, we reject the null hypothesis. If the **p-value** is greater than the significance level, we fail to reject the null hypothesis.\n\n::: column-margin\nClick [here](https://towardsdatascience.com/a-simple-interpretation-of-p-values-34db3777d907) to read more about p-values and significance levels.\n:::\n\nThese decisions can be either correct or incorrect. A correct decision occurs when we reject the null hypothesis when the null hypothesis is false or when we fail to reject the null hypothesis when the null hypothesis is true. An incorrect decision occurs when we reject the null hypothesis when the null hypothesis is true or when we fail to reject the null hypothesis when the null hypothesis is false.\n\nAll the possible outcomes of a hypothesis test are shown in the table below.\n\n| | $H_0$ is True | $H_0$ is False |\n|-----------------|-----------------|-----------------|\n| Do not reject $H_0$ | Correct Decision | Type II Error |\n| Reject $H_0$ | Type I Error | Correct Decision |\n\n# Type I Errors\n\nA Type I error occurs when we reject a true null hypothesis. In other words, we conclude that there is an effect when there is no effect. Type I errors are also known as false positives. The probability of making a Type I error is denoted by $\\alpha$ and is also known as the significance level. **We control the likelihood of making a Type I error** by setting the significance level. \n\nWhen we construct a 95% confidence interval, we set the significance level to 0.05. This indicates that there is a 5% chance of making a Type I error. $\\alpha = 0.05$ is the most common significance level used in hypothesis testing.\n\n::: column-margin\n**Quote**:  *\"For in fact no scientific worker has a fixed level of significance at which from year to year, and in all circumstances, he rejects hypotheses; he rather gives his mind to each particular case in the light of his evidence and his ideas\"* - Ronald Fisher\n\nKeep in mind that the significance level can be adjusted based on the context of the hypothesis test and what type of error is more costly. Alphas of 0.05 or 0.01 are not the end-all-be-all significance levels.\n:::\n\nIn our example of the new training facilities, a Type I error would occur if we conclude that teams with new training facilities have a higher points per match played than teams without new training facilities, when there truly is no difference in points per match played between the two groups.\n\n::: {.callout-note title=\"Exercise 1: Type I Errors\"}\n\na. What are some potential consequences of making a Type I error in the context of the new training facilities?\n\nb. Would a Type I error in this context have greater consequences for the team's finances or the team's performance?\n\nc. If alpha = 0.05, what is the percent chance that there is no difference in points per match played between the two groups given that we accept the alternative hypothesis of greater points per match played for teams with new training facilities?\n\nd. If alpha = 0.01, what is the percent chance of making a Type I error, given we reject the null hypothesis?\n\ne. If alpha = 0.01, what is the likelihood of making a Type I error if we fail to reject the null hypothesis?\n\nf. Is there a best significance level to always use in hypothesis testing? Why or why not?\n\n:::\n\n# Type II Errors\n\nA Type II error occurs when we fail to reject a false null hypothesis. We conclude that there is no effect when there is an effect. Type II errors are also known as false negatives. The probability of making a Type II error is denoted by $\\beta$. The value of beta is determined by the power of the test, which is the probability of correctly rejecting a false null hypothesis. Often we want power to be at least 0.80. This means that we want the probability of making a Type II error to be less than 0.20.\n\n$$\\beta = 1 - \\text{Power}$$\nThe power of a test depends on the sample size, the effect size, and the significance level. As a general rule, increasing the sample size, increasing the effect size, and decreasing the significance level will increase the power of a test. The full calculations for power are beyond the scope of this lesson.\n\n\n::: {.cell}\n\n:::\n\n\n\nIn our example of the new training facilities, a Type II error would occur if we conclude that there is no difference in points per match played between teams with new training facilities and teams without new training facilities, when in reality teams with new training facilities have a higher points per match played than teams without new training facilities.\n\n::: column-margin\n\nThe effect size of our test for comparing points per match played between teams with new training facilities and teams without new training facilities is found using [Cohens-d](https://resources.nu.edu/statsresources/cohensd) and the power is then calculated using more advanced statistical methods. \n\n:::\n\n\n::: {.callout-note title=\"Exercise 2: Type II Errors\"}\n\na. What are some potential consequences of making a Type II error in the context of the new training facilities?\n\nb. Would a Type II error in this context have greater consequences for the team's finances or the team's performance?\n\nPower can sometimes be hard to come by with small samples in hypothesis testing. For example, using the points per match played in 2023 for teams with new training facilities and teams without new training facilities we have 17 teams with new training facilities and 12 teams without new training facilities. With these sample sizes, the samples' effect size, and an $\\alpha = 0.5$, we have a power of 0.46269. \n\nc. What is the probability of making a Type II error in this instance, given we fail to reject the null hypothesis?\n\nd. What is the probability of making a Type II error in this instance, given we reject the null hypothesis?\n\ne. What could be done to increase the power of the test?\n\n::: \n\n# Relationship between Type I and Type II Errors\n\nThere is a trade-off between Type I and Type II errors. If we decrease the probability of making a Type I error, we increase the probability of making a Type II error. If we increase the probability of making a Type I error, we decrease the probability of making a Type II error. This means that we need to carefully consider the consequences of each type of error when designing a hypothesis test. \n\nIn our example of the new training facilities, if we increase the significance level from 0.05 to 0.10, our power increases from 0.463 to 0.596. This means that we are more likely to detect a difference in points per match played between teams with new training facilities and teams without new training facilities. However, we are also twice as likely to make a Type I error than previously.\n\nBelow is a plot showing this trade-off between Type I and Type II errors in our example of the new training facilities. \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nIn the plot above it can be seen that we never have as much power as we would like for any reasonable Type I error rate. \n\n::: {.callout-note title=\"Exercise 3: Relationship between Type I and Type II Errors\"}\n\na. Does sample size affect the probability of making a Type I or Type II error?\n\nb. Does changing the significance level affect the probability of making a Type I or Type II error?\n\nc. If we perform a test at the 0.01 significance level and another test at the 0.05 significance level, which test is more likely to result in a Type I error? Which test is more likely to result in a Type II error?\n\n:::\n\n## What's Worse: Type I or Type II Errors?\n\nThe answer to this question depends on the context of the hypothesis test. In some cases, a Type I error is more serious than a Type II error. In other cases, a Type II error is more serious than a Type I error. Below are two examples related to injuries in sports.\n\n#### Example where Type I error is more serious than Type II error\n\nAssume that a new recovery method is being tested for athletes with muscle injuries. This method claims to help athletes recover faster from their injuries. The null hypothesis is that the new recovery method takes the same amount of time as traditional recovery methods. The alternative hypothesis is that the new recovery method is faster than traditional recovery methods.\n\nA Type I error in this context would claim that the new recovery method is effective, when in reality it is not. This could result in athletes using the new recovery method and potentially worsening their injuries by returning to play too soon. A Type II error in this context would be when the new recovery method is assumed to be ineffective when it is actually effective. This would result in athletes continuing to recover following traditional recovery timelines.\n\nThe risk of more serious injuries and more setbacks in the injury recovery process appears to be more serious than the risk of athletes continuing to follow standard recovery timelines. Therefore, in this context, a Type I error is more serious than a Type II error.\n\n#### Example where Type II error is more serious than Type I error\n\nAn example of a Type II error being more serious than a Type I error can be seen in the context of concussion protocols in sports. A player takes a hard hit to the head during a game. Team physicians run quick tests on the player. The null hypothesis is that the player performs equal on the test to an uninjured player and therefore does not have a concussion, while the alternative hypothesis is that the player performs worse and therefore has a concussion.\n\nA Type I error would assume a player has a concussion when they do not. This could result in the player being taken out of the game unnecessarily, but would not have long term consequences for their health and safety. A Type II error in this context would incorrectly assume a player was fine after a head injury, when they in fact have a concussion. This could result in the player continuing to play when they should not, potentially leading to serious long-term brain damage.\n\nThe risk of serious long term brain damage from a concussion is much greater concern than the risk of a player being taken out of a game unnecessarily. Therefore, in this context, a Type II error is more serious than a Type I error.\n\n::: {.callout-note title=\"Exercise 4: Consequences of Errors\"}\n\nAnswer the following questions in the context of the new training facilities:\n\na. Which type of error would be more likely to hurt/fail to help the performance of the team?\n\nb. Which type of error would be more costly financially?\n\nc. Which type of error do you think is more serious in this context: Type I or Type II?\n\n:::\n\n# Results of the Hypothesis Test\n\n\n::: {.cell}\n\n:::\n\n\nUsing our example of the new training facilities, we can perform a hypothesis test to determine if there is a difference in points per match played between teams with new training facilities and teams without new training facilities. Results of different hypothesis tests with different significance levels and power are shown below. The p-value of the test is 0.03524.\n\n::: column-margin\nThe test used to produce these results is a two-sample t-test for the means of two independent samples. For more information on tests like this click [here](https://www.itl.nist.gov/div898/handbook/eda/section3/eda353.htm).\n:::\n\nTest NO. | Significance Level |  Power | Decision | Possible Error | Possible Error Rate | \n|-------|--------------------|--------|--------|----------------|---------------------|\n|1| 0.05 | 0.463 | Reject Null Hypothesis | Type I Error | 0.05 |\n|2| 0.10 | 0.596 | Reject Null Hypothesis | Type I Error | 0.10 |\n|3| 0.01 | 0.224 | Fail to Reject Null Hypothesis | Type II Error | 0.776 |\n\n::: {.callout-note title=\"Exercise 5: Results of the Hypothesis Test\"}\n\na. Which hypothesis test results in a decision that has the highest chance of being an error?\n\nb. Which hypothesis test has the greatest chance of producing a Type I error?\n\nc. Which hypothesis test has the greatest chance of making a Type II error?\n\nd. If you were advising a team owner on whether to invest in new training facilities, what would you recommend?\n:::\n\n\n# Conclusion\n\nIn this lesson, we learned about Type I and Type II errors in hypothesis testing. We learned that a Type I error is a false positive, and the rate is controlled by the significance level of the test. We learned that a Type II error is a false negative, and the rate is controlled by the power of the test. There is a trade-off between Type I and Type II errors, and the consequences of each type of error depend on the context of the hypothesis test. \n\n::: column-margin\n\n**Fun Fact**: The Seattle Sounders have clearly bought into the idea that new training facilities will help them win more games. Their new facility opened in 2024 and they bid farewell to the old Starfire Training Facility depicted below.\n\n![Starfire Training Facility](https://upload.wikimedia.org/wikipedia/commons/a/a7/Starfire_Sports_Complex_-_stadium_field_01.jpg)\n\nImage Source: <a href=\"https://commons.wikimedia.org/wiki/File:Starfire_Sports_Complex_-_stadium_field_01.jpg\">Joe Mabel</a>, <a href=\"http://creativecommons.org/licenses/by-sa/3.0/\">CC BY-SA 3.0</a>, via Wikimedia Commons\n:::\n\nErrors in hypothesis testing can have serious consequences, so it is important to carefully consider the significance level and power of the test when designing a hypothesis test. Increasing the sample size is the most effective way to make our hypothesis test more powerful. Just remember, when performing and interpreting hypothesis tests there is always a chance of making an error, and it is important to consider the consequences of each type of error before beginning.",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}