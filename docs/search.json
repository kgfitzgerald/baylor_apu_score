[
  {
    "objectID": "by-statsds-topic.html",
    "href": "by-statsds-topic.html",
    "title": "Modules By Topic",
    "section": "",
    "text": "American Ninja Warrior - Kaplan-Meier Survival Analyis\n\n\n\n\n\n\nKaplan-Meier\n\n\nLog Rank test\n\n\nNonparametric tests\n\n\n\nExploring Survival Analysis using the Kaplan-Meier method with American Ninja Warrior data.\n\n\n\n\n\nJul 22, 2024\n\n\nJonathan Lieb\n\n\n\n\n\n\n\n\n\n\n\n\nMMA Fight Decisions\n\n\n\n\n\n\nVariable types\n\n\nChi-square test (2x2)\n\n\nFisher Exact test\n\n\nChi-square test (rxc)\n\n\nMosaic plots\n\n\n\nExploring MMA fight decisions by weight category\n\n\n\n\n\nJul 1, 2024\n\n\nRodney X. Sturdivant, Ph.D., Ian Young, Joshua Patrick, Ph.D., Connor Bryson\n\n\n\n\n\n\n\n\n\n\n\n\nPGA - Scheffler Greens in Regulation\n\n\n\n\n\n\nConfidence Intervals\n\n\nBinomial Tests\n\n\nComparing Two Proportions\n\n\n\nExploring Probability Confidence Intervals with Golf Data\n\n\n\n\n\nJul 18, 2024\n\n\nJonathan Lieb\n\n\n\n\n\n\n\n\n\n\n\n\nPGA - Scheffler Greens in Regulation (No R)\n\n\n\n\n\n\nSample Proportions\n\n\nSingle Proportion Confidence Intervals\n\n\nComparing Two Proportions\n\n\n\nExploring Probability Confidence Intervals with Golf Data\n\n\n\n\n\nJul 26, 2024\n\n\nJonathan Lieb\n\n\n\n\n\n\n\n\n\n\n\n\nPGA - Scoring Average Confidence Intervals\n\n\n\n\n\n\nSingle Mean Confidence Intervals\n\n\nSingle Mean Hypothesis Testing\n\n\n\nExploring Single Mean Confidence Intervals with Golf Data\n\n\n\n\n\nJul 18, 2024\n\n\nJonathan Lieb\n\n\n\n\n\n\n\n\n\n\n\n\nPGA - Scoring Average Confidence Intervals (No R)\n\n\n\n\n\n\nSingle Mean Confidence Intervals\n\n\nSingle Mean Hypothesis Testing\n\n\n\nExploring Single Mean Confidence Intervals with Golf Data\n\n\n\n\n\nJul 24, 2024\n\n\nJonathan Lieb\n\n\n\n\n\n\n\n\n\n\n\n\nTennis Sample Size\n\n\n\n\n\n\nSample size\n\n\nPower\n\n\nIndependent two sample t-test\n\n\nIndependent two sample test of proportions\n\n\nHypotheses testing\n\n\nTypes of errors\n\n\n\nComputing sample size for studies involving tennis\n\n\n\n\n\nJul 1, 2024\n\n\nRodney X. Sturdivant, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nVolleyball - Women’s NCAA Division I\n\n\n\n\n\n\nVariable types\n\n\nggplot basics\n\n\nExploratory Data Analysis (EDA)\n\n\nHistograms\n\n\nBar plots (simple & stacked)\n\n\nScatterplots\n\n\nRidge plots\n\n\nBoxplots\n\n\n\nExploring volleyball statistics through visualization\n\n\n\n\n\nMay 30, 2024\n\n\nKatie Fitzgerald, Jazmine Gurrola, Joseph Hsieh, Tuan Dat Tran\n\n\n\n\n\n\n\n\n\n\n\n\nWhat’s the prime age of an MLB player?\n\n\n\n\n\n\ndata wrangling\n\n\ndplyr basics\n\n\ntidyverse\n\n\n\nExploring the prime age of MLB players\n\n\n\n\n\nJul 26, 2024\n\n\nJazmine Gurrola, Joseph Hsieh, Dat Tran, Katie Fitzgerald\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home",
      "Modules By Topic"
    ]
  },
  {
    "objectID": "volleyball/ncaa_d1/index.html",
    "href": "volleyball/ncaa_d1/index.html",
    "title": "Volleyball - Women’s NCAA Division I",
    "section": "",
    "text": "Methods/Facilitation notes\n\n\n\n\n\n\nThis module would be suitable for an in-class lab or take-home assignment in an introductory statistics course.\nIt assumes a basic familiarity with the RStudio Environment has already been covered, but no prior programming experiences is expected.\nStudents should be provided with the following data file (.csv) and Quarto document (.qmd) to produce visualizations and write up their answers to each exercise. Their final deliverable is to turn in an .html document produced by “Rendering” the .qmd.\n\ndata\nStudent Quarto template\n\nPosit Cloud (via an Instructor account) or Github classroom are good options for disseminating files to students, but simply uploading files to your university’s course management system works, too.",
    "crumbs": [
      "Home",
      "Volleyball",
      "Volleyball - Women's NCAA Division I"
    ]
  },
  {
    "objectID": "volleyball/ncaa_d1/index.html#volleyball-lingo",
    "href": "volleyball/ncaa_d1/index.html#volleyball-lingo",
    "title": "Volleyball - Women’s NCAA Division I",
    "section": "Volleyball lingo",
    "text": "Volleyball lingo\nBefore proceeding with any analysis, let’s make sure we know some volleyball lingo in order to understand what information is contained in each variable (column) in our dataset.\n\n\nTotally new to volleyball? Watch this 4-minute video: The Rules of Volleyball - EXPLAINED!\n\nBe the first team to win 3 sets to 25 points!!\n Image source: BoxOut Sports\n\n\nThe basics\n\nTo win a volleyball match, your team must be the first to win 3 sets\nA match can consist of 3, 4, or 5 sets (“best 3 out of 5”)\nYour team wins a set if you are the first to score 25 points\n\nbut you have to win by at least 2 points!\nand the 5th match (if necessary) only goes to 15 points\n\nSo how do you score points? By hitting the ball into your opponents side of the court without them successfully returning the ball. (Or by them committing an unforced error such as a missed serve or running into the net, but we won’t worry about that in this analysis).\nPlay begins on each point with a serve from the back line and ends when the ball hits the ground.\n\n\n\n\n\n\n\nVolleyball “stats” that might occur during any given play\n\n\n\n\nAn ace is a serve that directly results in a point (the opponent does not successfully return the serve).\nAn attack is on offensive play where a player strategically hits the ball over the net using an overhead motion\nA kill is when an attack results in a point for the attacking team (the opposing team doesn’t “dig” or successfully return the ball)\nAn assist is recorded when a player passes or sets the ball to a teammate who attacks the ball for a kill. This is often done by a setter and is the 2nd touch of a play (after the pass, before the hit).\nA block is a defensive play at the net when a player successfully blocks the ball from an opponent’s attack, sending it back down into the opponent’s court, directly resulting in a point.\nA dig is a defensive play when a player successfully passes the ball from an attack and keeps it in play",
    "crumbs": [
      "Home",
      "Volleyball",
      "Volleyball - Women's NCAA Division I"
    ]
  },
  {
    "objectID": "volleyball/ncaa_d1/index.html#variable-descriptions",
    "href": "volleyball/ncaa_d1/index.html#variable-descriptions",
    "title": "Volleyball - Women’s NCAA Division I",
    "section": "Variable descriptions",
    "text": "Variable descriptions\nThe volleyball data you’ll be analyzing in this lab provides season-level team statistics for 334 teams during the 2022-3 season. Many of the variables are reported as an average “per set.” For example, if a team played 30 matches, this means they played anywhere from 90 to 150 sets during the season, so aces_per_set provides the number of aces they scored, on average, across those 90+ sets. The table below provides detailed descriptions of each variable.\n\n\nVariable Descriptions\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nteam\ncollege of the volleyball team\n\n\nconference\nconference to which the team belongs\n\n\nregion\nregion to which the team belongs\n\n\naces_per_set\nthe average amount of balls served that directly lead to a point (not including errors by the opponent) per set\n\n\nassists_per_set\nthe average amount of sets, passes, or digs to a teammate that directly result in a kill per set\n\n\nteam_attacks_per_set\nthe average amount of times the ball is sent to the other team’s court from an overhead motion per set\n\n\nblocks_per_set\nthe average amount of times the ball is blocked from being hit on to the teams side per set\n\n\ndigs_per_set\naverage amount of times the ball is passed by a player successfully after an opponents attack per set\n\n\nhitting_pctg\ntotal team kills minus team hitting errors all divided by total attempts\n\n\nkills_per_set\naverage amount of hits that directly result in a point per set\n\n\nopp_hitting_pctg\nthe average hitting percentage of the teams opponent per set\n\n\nw\nthe amount of team wins for the season\n\n\nl\nthe amount of team losses for the season\n\n\nwin_pctg\nthe amount of total wins divided by the total matches of the season\n\n\nwinning_season\nIndication (yes/no) of whether the team won 50% or more of its matches during the season",
    "crumbs": [
      "Home",
      "Volleyball",
      "Volleyball - Women's NCAA Division I"
    ]
  },
  {
    "objectID": "volleyball/ncaa_d1/index.html#viewing-your-data",
    "href": "volleyball/ncaa_d1/index.html#viewing-your-data",
    "title": "Volleyball - Women’s NCAA Division I",
    "section": "Viewing your data",
    "text": "Viewing your data\nYou saw that glimpse() is one way to get a quick look at your data. Often, you’ll want to view your whole dataset. There are two ways to do this:\n\n\n\nTIP: Recall that RStudio is split into four quadrants: Source (upper left), Environment (upper right), Console (bottom left), and Files / Plots / Packages / Help / Viewer (bottom right) \n\ntype View(volleyball) in your Console and then click return/Enter on your keyboard.\nOR, in your Environment tab, double click the name of the dataset you want to view.\n\nThis will open up your data in a new viewer tab so that you can view it like a spreadsheet (like Google Sheets or Excel*). Once open, you can sort the data by clicking on a column.\n\n\n*Unlike Google Sheets or Excel, however, you won’t be able to edit the data directly in the spreadsheet.\n\n\n\n\n\n\nExercise 2\n\n\n\nView the volleyball data and sort it appropriately to answer the following questions:\n\nWhich NCAA Division I women’s volleyball team had the best record (highest win percentage) during the 2022-3 season?\nWhat percentage of their matches did this team win?\nWhat conference and region is this team in?\nWhich team had the worst record?\nWhat percentage of their matches did they win?\nWhat conference and region is this team in?\n\n\n\n\n\nTIP: When viewing the data, clicking on a column once will sort the data according to that variable in ascending order; clicking twice will sort in descending order.",
    "crumbs": [
      "Home",
      "Volleyball",
      "Volleyball - Women's NCAA Division I"
    ]
  },
  {
    "objectID": "volleyball/ncaa_d1/index.html#creating-visualizations",
    "href": "volleyball/ncaa_d1/index.html#creating-visualizations",
    "title": "Volleyball - Women’s NCAA Division I",
    "section": "Creating visualizations",
    "text": "Creating visualizations\nR (and the tidyverse package in particular) has some powerful functions for making visualizations. The type of visualization you should create depends on the type(s) of variable(s) you are exploring. In the remainder of this module, you will explore the volleyball data via visualizations.",
    "crumbs": [
      "Home",
      "Volleyball",
      "Volleyball - Women's NCAA Division I"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Baylor-APU SCORE Module Pre-print Repository",
    "section": "",
    "text": "This page contains education materials for the SCORE Network that were created by faculty and students from the Department of Statistical Science at Baylor University and the Department of Mathematics, Physics, & Statistics at Azusa Pacific University.\nThe SCORE Network Module Repository enables you to search for modules by either sport (along the left), or you can browse by statistics and data science topic.\nPlease note that these material have not yet completed the required pedagogical and industry peer reviews to become a published module on the SCORE Network. However, instructors are still welcome to use these materials if they are so inclined.\nThe development of the SCORE with Data network is funded by the National Science Foundation (award 2142705).\nContributing and/or Joining the SCORE Network\n\nIf you are interested in contributing to and/or joining the SCORE Network, please check out https://scorenetwork.org/index.html.\nIf you are interested in creating a page similar to this to host your own “in development modules”, please feel free to copy the Github repository this page was derived from: https://github.com/iramler/slu_score_preprints."
  },
  {
    "objectID": "tennis/tennis_sampsize/index.html",
    "href": "tennis/tennis_sampsize/index.html",
    "title": "Tennis Sample Size",
    "section": "",
    "text": "Facilitation notes\n\n\n\n\n\n\nThis module would be suitable for an in-class lab or take-home assignment in an introductory statistics course.\nIt assumes a basic familiarity with the RStudio Environment has already been covered, but no prior programming experiences is expected.\nStudents should be provided with the following data file (.csv) and Quarto document (.qmd) to produce tests/visualizations and write up their answers to each exercise. Their final deliverable is to turn in an .html document produced by “Rendering” the .qmd.\n\ndata\nStudent Quarto template\n\nPosit Cloud (via an Instructor account) or Github classroom are good options for disseminating files to students, but simply uploading files to your university’s course management system works, too.",
    "crumbs": [
      "Home",
      "Tennis",
      "Tennis Sample Size"
    ]
  },
  {
    "objectID": "tennis/tennis_sampsize/index.html#tennis-data-overview",
    "href": "tennis/tennis_sampsize/index.html#tennis-data-overview",
    "title": "Tennis Sample Size",
    "section": "Tennis Data Overview",
    "text": "Tennis Data Overview\nBefore proceeding with any analysis, let’s make sure we understand what information is contained for key variables (column) in our dataset.\nThe data set is from the 2023 Men’s Singles Wimbledon Championships, perhaps the most important tennis tournament each year.\n\n\n\n\n\n\nBasic Features of the Wimbledon 2023 data\n\n\n\n\nWimbledon is a single elimination tournament\n\nRound 1 begins with 128 players; 64 matches are played with the 64 winning players advancing to the second round.\nSubsequent rounds are played until two players reach the final; the winner of this final round is the Wimbledon Champion\n\nOur tennis data set includes all matches after the second round.\nThe data provides information for every point played in the matches\n\nEach row represents a point\nPoints are ordered within each match from the first to last point\n\n\n\n\n\n\nTotally new to Tennis? See this site: INTRODUCTION TO TENNIS SCORING\n\nFor more information about Wimbledon: Wimbledon Official Site",
    "crumbs": [
      "Home",
      "Tennis",
      "Tennis Sample Size"
    ]
  },
  {
    "objectID": "tennis/tennis_sampsize/index.html#variable-descriptions",
    "href": "tennis/tennis_sampsize/index.html#variable-descriptions",
    "title": "Tennis Sample Size",
    "section": "Variable descriptions",
    "text": "Variable descriptions\nWe will actually only use a few columns for this module, but the full description of the data is provided. Some variables have data for both players with columns with labels starting “p1” for player 1 and “p2” for player two. We define these for player 1, but the definitions hold for the corresponding player 2 variables.\n\n\n\n\n\n\n\n\n\n\nVariable\nDefinition\n\n\n\n\nmatch_id\nmatch identification\n\n\nplayer1\nfirst and last name of the first player\n\n\nplayer2\nfirst and last name of the second player\n\n\nelapsed_time\ntime elapsed since start of first point to start of current point (H:MM:SS)\n\n\nset_no\nset number in match\n\n\ngame_no\ngame number in set\n\n\npoint_no\npoint number in game\n\n\np1_sets\nsets won by player 1\n\n\np1_games\ngames won by player 1 in current set\n\n\np1_score\nplayer 1's score within current game\n\n\nserver\nserver of the point\n\n\nserve_no\nfirst or second serve\n\n\npoint_victor\nwinner of the point\n\n\np1_points_won\nnumber of points won by player 1 in match\n\n\ngame_victor\na player won a game this point\n\n\nset_victor\na player won a set this point\n\n\np1_ace\nplayer 1 hit an untouchable winning serve\n\n\np1_winner\nplayer 1 hit an untouchable winning shot\n\n\nwinner_shot_type\ncategory of untouchable shot\n\n\np1_double_fault\nplayer 1 missed both serves and lost the point\n\n\np1_unf_err\nplayer 1 made an unforced error\n\n\np1_net_pt\nplayer 1 made it to the net\n\n\np1_net_pt_won\nplayer 1 won the point while at the net\n\n\np1_break_pt\nplayer 1 has an opportunity to win a game player 2 is serving\n\n\np1_break_pt_won\nplayer 1 won the game player 2 is serving\n\n\np1_break_pt_missed\nplayer 1 missed an opportunity to win a game player 2 is serving\n\n\np1_distance_run\nplayer 1's distance ran during point (meters)\n\n\nrally_count\nnumber of shots during the point\n\n\nspeed_mph\nspeed of serve (miles per hour; mph)\n\n\nserve_width\ndirection of serve\n\n\nserve_depth\ndepth of serve\n\n\nreturn_depth\ndepth of return",
    "crumbs": [
      "Home",
      "Tennis",
      "Tennis Sample Size"
    ]
  },
  {
    "objectID": "tennis/tennis_sampsize/index.html#viewing-your-data",
    "href": "tennis/tennis_sampsize/index.html#viewing-your-data",
    "title": "Tennis Sample Size",
    "section": "Viewing your data",
    "text": "Viewing your data\nYou saw that glimpse() is one way to get a quick look at your data. Often, you’ll want to view your whole dataset. There are two ways to do this:\n\n\n\nTIP: Recall that RStudio is split into four quadrants: Source (upper left), Environment (upper right), Console (bottom left), and Files/Plots/Packages/Help/Viewer (bottom right)\n\ntype View(tennis) in your Console and then click return/Enter on your keyboard.\nOR, in your Environment tab, double click the name of the dataset you want to view.\n\nThis will open up your data in a new viewer tab so that you can view it like a spreadsheet (like Google Sheets or Excel*). Once open, you can sort the data by clicking on a column.\n\n\n*Unlike Google Sheets or Excel, however, you won’t be able to edit the data directly in the spreadsheet.\n\n\n\n\n\n\nExercise 1\n\n\n\nView the tennis data and sort it appropriately to answer the following questions:\n\nWe will be interested in the distances run. Which variables in the data set contain this data? What are example values (and units) for this data? What are the smallest and largest values?\nThe second research question involves the percentage of points won on the first serve. What variables provide information related to this question? What type of variables are they and what are the possible values for each?\n\n\n\n\n\n\nTIP: Type your answers to each exercise in the .qmd document.\n\n\n\n\n\n\nTIP: When viewing the data, clicking on a column once will sort the data according to that variable in ascending order; clicking twice will sort in descending order.",
    "crumbs": [
      "Home",
      "Tennis",
      "Tennis Sample Size"
    ]
  },
  {
    "objectID": "tennis/tennis_sampsize/index.html#why-does-sample-size-matter",
    "href": "tennis/tennis_sampsize/index.html#why-does-sample-size-matter",
    "title": "Tennis Sample Size",
    "section": "Why does sample size matter",
    "text": "Why does sample size matter\nThere are several things that drive the need for sample size calculations. One is resources. Often it is costly - financially, time, difficulty in getting data - to conduct an experiment so we wish to do so efficiently, with the smallest sample possible. The second is that we want a sample size that will ensure we can get meaningful results from our study. The last thing anyone wants after spending time and money on research is for the results to be inconclusive.\n\nTypes of errors\nTwo types of errors can occur with a statistical hypotheses test.\n\nType I error: the null hypothesis is true but we reject it.\nType II error: the null hypothesis is false but we fail to reject it.\n\n\n\n\n\n\n\nImportant\n\n\n\nThe errors are a trade off - if we improve one, the other is worse.\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\nFor our proposed study of a method to improve first serve perentage:\n\nWhat would it mean to have a Type I error?\nWhat would it mean to have a Type II error?\nIs one error “worse” than the other in this case? Explain.\n\n\n\n\n\nBalancing the errors\nOur goal is to maintain reasonably small chances of both types of errors. The Type I error is typically handled by specifying the probability we make such an error in our hypothesis testing procedure. This probability is usually referred to as \\(\\alpha\\) (“alpha”) and set to a low value. Most typically we use \\(\\alpha = 0.05\\). The Type II error, on the other hand, is NOT specified in the testing. This is where sample size can play a role.\nReturning to our example, suppose we collect a sample and the first serve percentage without the training is 50% and after the training it is 75%. Using an \\(\\alpha = 0.05\\) value, however, we do not reject the null hypothesis. We cannot conclude the sample provides evidence of improvement…even though it seems like a rather positive effect!\nOur sample was from two games with 4 first serves each…a very small sample. The problem is that with such a small sample we lack power to detect a difference even if it exists. Power is defined as:\n\\[Power = 1 - \\beta\\] Where \\(\\beta\\) is the Type II error rate. Thus, we controlled Type I error but our Type II error rate may be too high!\nSmall samples lead to large uncertainty about the estimates. Consider the 50% estimate for without training. That was based on 2 of 4 successful serves. However, if only one of the serves had been different (say one more success) that percentage would change by 25%!\n\n\n\n\n\n\nImportant\n\n\n\nA study that has too small of a sample size to detect a meaningful effect is said to be under powered. The Type II error rate is very high.\n\n\nWe thus want a sample large enough that it will reject the null hypothesis when there is a true effect. Generally speaking, resource constraints lead to trying to find the smallest sample size that has adequate power to do so. There is little danger of getting “too large” a sample. You might wonder, though, if resources permit why not just get a super large sample. That would give very high power!\nThe problem with a very large sample is that there is power to detect very, very small effects. For example, suppose we get a sample of millions of serves. The result is 50% success without the training and 50.1% with the training. The gigantic sample could lead to rejecting the null hypothesis, thus concluding there is evidence of difference due to the training.\nThe sample size gives us great precision in these estimates…and, after, all technically they are different. However, clearly the effect is not really a difference that any tennis play would care about enough to hire you as their trainer.\n\n\n\n\n\n\nImportant\n\n\n\nA study that has too large of a sample size to detect a meaningful effect is said to be over powered. The Type II error rate is so low that meaningless effects are said to be significant.",
    "crumbs": [
      "Home",
      "Tennis",
      "Tennis Sample Size"
    ]
  },
  {
    "objectID": "tennis/tennis_sampsize/index.html#factors-impacting-sample-size-power",
    "href": "tennis/tennis_sampsize/index.html#factors-impacting-sample-size-power",
    "title": "Tennis Sample Size",
    "section": "Factors impacting sample size (power)",
    "text": "Factors impacting sample size (power)\nIn order to determine the sample size that gets us into the “sweet spot” (not in the sense of hitting a tennis ball) there are four factors that are important in some form for all “power” calculations:\n\nThe sample size (or power)\n\n\nnote that sometimes we use the power and compute sample size, and sometimes the reverse. If power is used typical choices are \\(0.8\\) or \\(0.9\\) (meaning \\(\\beta = 0.2\\) or \\(\\beta = 0.1\\) Type II error rates).\n\n\nYour chosen Type I error rate\n\n\nTypically we use \\(\\alpha = 0.05\\).\n\n\nThe amount of variability in the data.\n\n\nVariability impacts the precision of estimates.\n\n\nHow big of a difference (or how strong of an association) you believe exists and is meaningful.\n\nItems 3 and 4 must be estimated in some fashion which is often a challenge for sample size calculation. They are often combined and referred to as an “effect size”. There are various measures of effect size in different settings with rules of thumb for what constitues small, medium, etc. effect sizes that are then used to compute the desired sample size.",
    "crumbs": [
      "Home",
      "Tennis",
      "Tennis Sample Size"
    ]
  },
  {
    "objectID": "tennis/tennis_sampsize/index.html#estimating-the-parameters",
    "href": "tennis/tennis_sampsize/index.html#estimating-the-parameters",
    "title": "Tennis Sample Size",
    "section": "Estimating the Parameters",
    "text": "Estimating the Parameters\nWith the statistical method defined we are then ready to determine the parameter values we will use to perform our sample size computations. We will compute sample size for a desired power, and set the first two parameters to typical values:\n\nPower = \\(0.8\\)\n\\(\\alpha = 0.05\\)\n\nThe variability is often estimated from pilot or previous data, or using information from previous studies. We will use our Wimbledon 2023 data. We can compute the standard deviations (sd) for player one run distances and also for player two run distances:\n\nsd(tennis$p1_distance_run)\n\n[1] 13.49286\n\nsd(tennis$p2_distance_run)\n\n[1] 13.60765\n\n\nBoth are similar and around 13.5 meters/point. So, we will choose this value:\n\nsd = 13.5 meters/point (estimated variability)\n\nThe final value we need is the value of the difference in run distance that we would consider meaningful. The mean values for the run distances in the Wimbledon data are:\n\nmean(tennis$p1_distance_run)\n\n[1] 14.00231\n\nmean(tennis$p2_distance_run)\n\n[1] 13.86924\n\n\nBoth are around 14 meters/point. What would represent a meaningful increase for the distance run on clay?\nOne tool that could help is to consider effect size. Cohen (1988) offers some advice. A metric known as Cohen’s D is one measure and is defined:\n\\[D = \\frac{\\mu_2 - \\mu_1}{sd}\\] where the sd is of the difference in means. If the average distance on clay increases by one standard deviation, then \\(D = 1\\). In other words, Cohen’s D is the increase in terms of the standard deviation. If \\(D = 0.5\\) that would be an increase of half of a standard deviation,\nAn increase of 1 standard deviation (13.5 meters/point) seems large as it would nearly double the average run per point.\nThe R package “effectsize” contains a function to provide an interpretation of Cohen’s D values. We provide interpretation for the one standard deviation increase (13.5 meters/point) and for smaller increases of 0.5 and 0.25 standard deviations.\n\ninterpret_cohens_d(1)\n\n[1] \"large\"\n(Rules: cohen1988)\n\ninterpret_cohens_d(0.5)\n\n[1] \"medium\"\n(Rules: cohen1988)\n\ninterpret_cohens_d(0.25)\n\n[1] \"small\"\n(Rules: cohen1988)\n\n\nWe will opt for a “medium” effect size using the \\(D = 0.5\\) value. That would be an increase of one half of a standard deviation: \\(0.5 \\times 13.5 = 6.75\\).\n\nDifference in means (delta) = 6.75.",
    "crumbs": [
      "Home",
      "Tennis",
      "Tennis Sample Size"
    ]
  },
  {
    "objectID": "tennis/tennis_sampsize/index.html#computing-the-sample-size",
    "href": "tennis/tennis_sampsize/index.html#computing-the-sample-size",
    "title": "Tennis Sample Size",
    "section": "Computing the Sample Size",
    "text": "Computing the Sample Size\nThe R command “power.t.test” computes the sample size (or power). We compute sample size by leaving the parameter “n” as “NULL” (note that NULL is the default so we did not need to explicitly specify in running the commmand). We must specify the power in this case; alternatively we could give a value of “n” and make the power NULL to compute power.\nOther values are shown below.\nNote that the type is “two.sample” because we are comparing two sample means.\nThe alternative is “one.sided” because we hypothesized and increase (&gt;) in distance run on clay courts. A \\(\\ne\\) alternative hypothesis would be “two.sided”.\n\npower.t.test(n = NULL, \n             delta = 6.76, \n             sd = 13.5, \n             sig.level = 0.05,\n             power = 0.8,\n             type = c(\"two.sample\"),\n             alternative = c(\"one.sided\")\n             )\n\n\n     Two-sample t test power calculation \n\n              n = 50.00462\n          delta = 6.76\n             sd = 13.5\n      sig.level = 0.05\n          power = 0.8\n    alternative = one.sided\n\nNOTE: n is number in *each* group\n\n\nThe value returned is n = 50.00462 which is the number per group (so points observed on each of the two court surfaces). We always round up to ensure adequate power so we will need n = 51 points per court surface to conduct our study.\n\n\n\n\n\n\nExercise 3\n\n\n\nIn the next exercises, we will examine how various parameters impact the required sample size. Let’s look first at the impact of power.\n\nRerun the sample size calculation increasing the desired power to 0.9. What sample size is required?\nRerun the sample size calculation decreasing the desired power to 0.7. What sample size is required?\nDoes increasing the power impact the required sample size? Why?\n\n\n\n\n\nTIP: copy the command in our example and change the parameters as needed to complete each exercise.\n\n\n\n\n\n\nExercise 4\n\n\n\nImpact of (alpha). Return to a power of 0.8 for this exercise.\n\nRerun the sample size calculation decreasing the desired alpha to 0.01. What sample size is required?\nRerun the sample size calculation increasing the desired alpha to 0.1. What sample size is required?\nHow does changing the allowable Type I error impact the required sample size? Why?\n\n\n\n\n\n\n\n\n\nExercise 5\n\n\n\nImpact of variability, sd. Return to a power of 0.8 and alpha of 0.05\n\nIncrease the sd to 15. What sample size is required?\nIncrease the sd to 10. What sample size is required?\nHow does the estimated variability in the data impact the required sample size? Why?\n\n\n\n\n\n\n\n\n\nExercise 6\n\n\n\nImpact of size of the difference, delta. Return to a power of 0.8, alpha of 0.05, and sd of 13.5.\n\nWhat value of delta leads to a small effect of D = 0.25 standard deviation increase?\nWhat value of delta produces a large effect of D = 1 sd increase?\nFind the sample size for the delta values computed in a and b.\nHow does the effect size impact the required sample size? Why?\n\n\n\n\n\n\n\n\n\nExercise 7\n\n\n\nComputing power instead of sample size. Use the original values in the example (delta = 6.75, sd = 13.5, alpha = 0.05). We computed a sample size of 51 to achieve power of 0.8 in the example.\n\nModify the command by setting power to “NULL” and the sample size to n = 51. Does the computed power exceed 0.8?\nWhat is the power if n = 50 (recall we rounded up)?",
    "crumbs": [
      "Home",
      "Tennis",
      "Tennis Sample Size"
    ]
  },
  {
    "objectID": "tennis/tennis_sampsize/index.html#estimating-the-parameters-1",
    "href": "tennis/tennis_sampsize/index.html#estimating-the-parameters-1",
    "title": "Tennis Sample Size",
    "section": "Estimating the Parameters",
    "text": "Estimating the Parameters\nWe will again compute sample size for a desired power, and set the first two parameters to typical values:\n\nPower = \\(0.8\\)\n\\(\\alpha = 0.05\\)\n\nThe third parameter, estimate of variability, is not needed for the two sample proportions test. The reason is that the variance for a proportion is actually a function of the proportion. This comes from the variance for a binary (0 or 1) variable which is modeled using the Binomial (Bernoulli) distribution. If the true proportion is p, then the variance is:\n\\[p \\times (1-p)\\]\nSo, once we provide a hypothesized value for p, then the variance can be computed!\nThe fourth parameter is again the difference we would consider meaningful. We can use our data to get an estimate for the proportion of points won on grass for a first serve.\nWe first get the percentage for player one. The “filter” function allows us to select only first serve data (“serve_no ==1”) when player one is serving (“server == 1”). We then obtain the table with percentages of which player won the point.\n\np1serve1 &lt;- tennis |&gt; filter(serve_no == 1 & server == 1)\nPercTable(p1serve1$point_victor)\n\n               \n    freq   perc\n               \n1  1'728  76.6%\n2    529  23.4%\n\n\nSince player 1 is the server in this reduced data set, we see that the server wins 76.6% of the points.\nWe can repeat this for player two (below) and find a similar percentage of 74.3%.\n\np2serve1 &lt;- tennis |&gt; filter(serve_no == 1 & server == 2)\nPercTable(p2serve1$point_victor)\n\n               \n    freq   perc\n               \n1    616  25.7%\n2  1'784  74.3%\n\n\nWe select a reasonable percentage then for Wimbledon (grass) of 75%. The question is what would be a noteworthy difference in winning percentage on clay.\nWe can again consider effect size, and R package “pwr” provides a function “ES.h” that computes an effect size based on two proportions known as Cohen’s H (Cohen, 1988). The rules of thumb for this value are the same as for Cohen’s D, so we can again use the “interpret_cohens_d” function once we obtain a value.\nLet’s see what the effect size is if the percentage won on clay is only 50%:\n\nprop_effect &lt;- ES.h(0.75, 0.5)\nprop_effect\n\n[1] 0.5235988\n\ninterpret_cohens_d(prop_effect)\n\n[1] \"medium\"\n(Rules: cohen1988)\n\n\nThe result is a “medium” effect, but practically that seems like an unlikely change. Even though clay might reduce the serve advantage, it still probably exists. Let’s consider reducing the advantage to 65%:\n\nprop_effect &lt;- ES.h(0.75, 0.65)\nprop_effect\n\n[1] 0.2189061\n\ninterpret_cohens_d(prop_effect)\n\n[1] \"small\"\n(Rules: cohen1988)\n\n\nThis is a small effect size, but practically certainly meaningful so we will use this in our calculations:\n\nDifference in proportions: 0.1 (from 0.75 to 0.65)",
    "crumbs": [
      "Home",
      "Tennis",
      "Tennis Sample Size"
    ]
  },
  {
    "objectID": "tennis/tennis_sampsize/index.html#computing-the-sample-size-1",
    "href": "tennis/tennis_sampsize/index.html#computing-the-sample-size-1",
    "title": "Tennis Sample Size",
    "section": "Computing the Sample Size",
    "text": "Computing the Sample Size\nThe command for proportions is “power.prop.test”. For difference in proportions, we actually input the two proportions rather than the delta. As we will see in the exercises, this matters as the estimate of the variance is based on the hypothesized proportion. The rest of the options are similar to those for the two sample t-test.\n\npower.prop.test(n = NULL, \n                p1 = 0.75, \n                p2 = 0.65, \n                sig.level = 0.05, \n                power = 0.8,\n                alternative = \"one.sided\")\n\n\n     Two-sample comparison of proportions power calculation \n\n              n = 258.619\n             p1 = 0.75\n             p2 = 0.65\n      sig.level = 0.05\n          power = 0.8\n    alternative = one.sided\n\nNOTE: n is number in *each* group\n\n\nThe value returned is n = 258.619 per group so we will need n = 259 points per court surface to conduct this second study.\n\n\n\n\n\n\nExercise 8\n\n\n\nAs mentioned in the example, the hypothesized proportion impacts the estimate of the variance. We explore this impact in this exercise.\n\nModify the sample size calculation so that the effect is still 0.1 but based on 70% for p1 and 60% for p2. What is the resulting sample size? How does this compare to the sample size in the example using 75% and 60%?\nThe variance estimate is related to the value \\(p \\times (1-p)\\). What is this value if p = 0.75? What is the value when p = 0.7? Which variance is larger?\nBased on the results from part b, and from your exploration of the role of variability in sample size calculations for the two sample t-test, explain the change in sample size in part a.\n\n\n\n\n\n\n\n\n\nExercise 9\n\n\n\nFormulate an additional question that involves two sample means using variables available in the data set and compute sample size in similar fashion to example 1.\n\n\n\n\n\n\n\n\nExercise 10\n\n\n\nFormulate an additional question that involves two sample proportions using variables available in the data set and compute sample size in similar fashion to example 2.",
    "crumbs": [
      "Home",
      "Tennis",
      "Tennis Sample Size"
    ]
  },
  {
    "objectID": "golf/pga_gir/index.html",
    "href": "golf/pga_gir/index.html",
    "title": "PGA - Scheffler Greens in Regulation",
    "section": "",
    "text": "Facilitation notes\n\n\n\n\n\n\nThis module would be suitable for an in-class lab or take-home assignment in an introductory statistics course.\nIt assumes a basic familiarity with the RStudio Environment has already been covered, but no prior programming experiences is expected.\nStudents should be provided with the following data file (.csv) and Quarto document (.qmd) to produce visualizations and write up their answers to each exercise. Their final deliverable is to turn in an .html document produced by “Rendering” the .qmd.\n\ndata\nStudent Quarto template\n\nPosit Cloud (via an Instructor account) or Github classroom are good options for disseminating files to students, but simply uploading files to your university’s course management system works, too.",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scheffler Greens in Regulation"
    ]
  },
  {
    "objectID": "golf/pga_gir/index.html#terms-to-know",
    "href": "golf/pga_gir/index.html#terms-to-know",
    "title": "PGA - Scheffler Greens in Regulation",
    "section": "Terms to know",
    "text": "Terms to know\nBefore proceeding with the analysis, let’s make sure we know some golf terminology that will help us putt-putt our way through this lab.\n\n\nAre Greens in Regulation important to scoring? Check out the table below to see how greens in regulation and lower handicaps go hand and hand  Image source: The Range by The Grint\n\nA Par 3 hole should take 1 shot to reach the green in regulation \nA Par 4 hole should take 2 shots to reach the green in regulation \nA Par 5 hole should take 3 shots to reach the green in regulation  Images source: Tanglewood Golf Course\n\n\nLie Terminology\n\nThe fairway is the short grass between the tee box and the green, where the ball is supposed to be hit on a par 4 or par 5 hole\nA bunker is a hazard filled with sand\n\nA fairway bunker is a bunker located in or next to the fairway\n\n\n\n\n\n\n\n\nPars and Greens in Regulation\n\n\n\n\nA par in golf is the number of strokes a good golfer is expected to take on a hole or course\nA green in regulation (GIR) is when a golfer reaches the green in the expected number of strokes (or fewer) on a par 3, 4, or 5 hole\n\nFor example, on a par 4 hole, a golfer would be expected to reach the green in 2 strokes (a drive and an approach shot) and then putt out in 2 more strokes for a total of 4 strokes. If a golfer reaches the green in 2 strokes, they have hit the green in regulation.\nOn a par 3 hole, a golfer would be expected to reach the green in 1 stroke\nOn a par 5 hole, a golfer would be expected to reach the green in 3 strokes.",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scheffler Greens in Regulation"
    ]
  },
  {
    "objectID": "golf/pga_gir/index.html#variable-descriptions",
    "href": "golf/pga_gir/index.html#variable-descriptions",
    "title": "PGA - Scheffler Greens in Regulation",
    "section": "Variable descriptions",
    "text": "Variable descriptions\nThe scheffler_2023 data you’ll be analyzing in this lab provides approach data for Scottie Scheffler in 2023. The data includes the number of greens in regulation (GIR) hit by Scottie Scheffler, the number of holes he played, and the cut he hit from.\n\n\nVariable Descriptions\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nplayer\nPGA Tour player\n\n\nyear\nPGA Tour Season\n\n\ngreens\nGreens in Regulation Hit\n\n\nholes\nNumber of Holes\n\n\ncut\nThe type of ground that the approach shot to the green was hit from",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scheffler Greens in Regulation"
    ]
  },
  {
    "objectID": "golf/pga_gir/index.html#mutating-data",
    "href": "golf/pga_gir/index.html#mutating-data",
    "title": "PGA - Scheffler Greens in Regulation",
    "section": "Mutating Data",
    "text": "Mutating Data\n\nMutating data in R is the process of creating new variables based on existing variables in a dataset. This can be done using the mutate() function in the dplyr package.\nThe mutate function takes a data frame as its first argument and then a series of new variable assignments. (Eg: mutate(data, new_var1 = old_var * 2, new_var = old_var1 - old_var2))",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scheffler Greens in Regulation"
    ]
  },
  {
    "objectID": "golf/pga_gir/index.html#arranging-data",
    "href": "golf/pga_gir/index.html#arranging-data",
    "title": "PGA - Scheffler Greens in Regulation",
    "section": "Arranging Data",
    "text": "Arranging Data\n\nThe arrange() function in the dplyr package is used to sort the data frame by one or more variables.\nThe arrange() function takes a data frame as its first argument and then a series of variables to sort by. (Eg: arrange(data, variable1, variable2))\nBy default, the arrange() function sorts in ascending order. To sort in descending order, you can use the desc() function inside the arrange() function. (Eg: arrange(data, desc(variable1)))\n\n\n\n\n\n\n\nExercise 2: Data Manipulation\n\n\n\n\nUse the mutate() function to create a new variable called gir_rate that represents the proportion of greens hit in regulation. Save this new dataset as scheffler_2023_mutated. Then arrange the scheffler_2023_mutated data by the gir_rate variable in descending order.\n\nFrom what cut did Scottie Scheffler have the best Green in Regulation rate?\nFrom what cut did Scottie Scheffler have the worst Green in Regulation rate?\n\n\n\n\nHINT: Values can be assigned to variable names in R using the &lt;- operator. For example, x &lt;- 2, y &lt;- \"Hello\", or z &lt;- 2 * x.\nUsing = also works in R for assigning values to variables, but it is common practice to use &lt;- for this purpose.",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scheffler Greens in Regulation"
    ]
  },
  {
    "objectID": "golf/pga_gir/index.html#making-confidence-intervals-in-r",
    "href": "golf/pga_gir/index.html#making-confidence-intervals-in-r",
    "title": "PGA - Scheffler Greens in Regulation",
    "section": "Making Confidence Intervals in R",
    "text": "Making Confidence Intervals in R\nThe statistical notation for a confidence interval for a proportion is:\n\\[\n\\hat{p} \\pm z \\times \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n\\]\nWhere:\n\n\\(\\hat{p}\\) is the sample proportion.\n\\(z\\) is the z-score that corresponds to the desired level of confidence\n\\(n\\) is the sample size\n\n\n\nTIP: The z-score for a 95% confidence interval is approximately 1.96. You can use this value to calculate the confidence interval for the proportion of greens hit in regulation by Scottie Scheffler from the fairway.\n\nSome common Confidence Intervals and their corresponding z-scores are:\n\n\n\nConfidence Interval\nZ-Score\n\n\n\n\n90 %\n1.65\n\n\n95 %\n1.96\n\n\n98 %\n2.33\n\n\n99 %\n2.58\n\n\n\n\nIn R you can perform basic math using the * operator for multiplication and the \\ operator for division.\nFor a quick example if the proportion of success for a problem is .6, the sample size is 100, and the test is being performed at the 95% confidence level, then the confidence interval for the proportion of success can be calculated as follows:\n\nlower_limit &lt;- .6 - 1.96 * sqrt((.6 * (1-.6))/100)\nupper_limit &lt;- .6 + 1.96 * sqrt((.6 * (1-.6))/100)\n\nThis would give you the lower limit (0.50398) and the upper limit (0.69602) of the 95% confidence interval for the true proportion of success.\n\n\n\n\n\n\nExercise 3: Manual Confidence Intervals\n\n\n\nUse the numbers from the scheffler_2023_mutated data frame where cut = “Fairway” to calculate a 95% confidence interval for the proportion of greens hit in regulation by Scottie Scheffler from the fairway.\n\nWhat is value of \\(\\hat{p}\\) for the fairway cut?\nWhat is the value of \\(n\\) for the fairway cut?\nWhat is the estimated value of the upper limit of the confidence interval for the fairway cut?\nWhat is the estimated value of the lower limit of the confidence interval for the fairway cut?",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scheffler Greens in Regulation"
    ]
  },
  {
    "objectID": "golf/pga_gir/index.html#using-binom.test-to-calculate-confidence-intervals",
    "href": "golf/pga_gir/index.html#using-binom.test-to-calculate-confidence-intervals",
    "title": "PGA - Scheffler Greens in Regulation",
    "section": "Using binom.test to Calculate Confidence Intervals",
    "text": "Using binom.test to Calculate Confidence Intervals\nIn R the binom.test() function can be used to calculate confidence intervals for proportions. The binom.test() function takes the number of successes and the total number of trials as arguments and returns a confidence interval for the proportion.\n\n\nTIP: You can type ?binom.test in the R console to get more information about the binom.test() function, including which arguments it takes and what it returns.\nBelow is an example of how to use the binom.test() function to calculate a 95% confidence interval for data where the number of successes is 60 and the total number of trials is 100. If $conf.int is added to the end of the binom.test() function, the function will return only the confidence interval and confidence level.\n\nbinom.test(60, 100, conf.level = .95)\n\n\n    Exact binomial test\n\ndata:  60 and 100\nnumber of successes = 60, number of trials = 100, p-value = 0.05689\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.4972092 0.6967052\nsample estimates:\nprobability of success \n                   0.6 \n\nbinom.test(60, 100, conf.level = .95)$conf.int\n\n[1] 0.4972092 0.6967052\nattr(,\"conf.level\")\n[1] 0.95\n\n\n\n\nTIP: The $ operator is used to access a specific element of a list in R. In this case, the conf.int element of the list returned by the binom.test() function contains the confidence interval.\n\n\n\n\n\n\nExercise 4: Using R Functions for Confidence Intervals\n\n\n\nUse the binom.test() function to calculate a 95% confidence interval for the proportion of greens hit in regulation by Scottie Scheffler from the bunker.\n\nWhat is the value of the lower limit of the confidence interval for the bunker cut?\nWhat is the value of the upper limit of the confidence interval for the bunker cut?\nIs the range of the confidence interval for the bunker cut wider or narrower than the range of the confidence interval for the fairway cut?",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scheffler Greens in Regulation"
    ]
  },
  {
    "objectID": "golf/pga_gir/index.html#testing-hypothesized-values",
    "href": "golf/pga_gir/index.html#testing-hypothesized-values",
    "title": "PGA - Scheffler Greens in Regulation",
    "section": "Testing Hypothesized Values",
    "text": "Testing Hypothesized Values\nHypothesis testing can also be done using the binom.test() function as seen below where it is being tested if the proportion of successes is .7 for a sample size of 100. The binom.test() function returns the p-value of the test.\n\nbinom.test(60, 100, p = .7)\n\n\n    Exact binomial test\n\ndata:  60 and 100\nnumber of successes = 60, number of trials = 100, p-value = 0.03745\nalternative hypothesis: true probability of success is not equal to 0.7\n95 percent confidence interval:\n 0.4972092 0.6967052\nsample estimates:\nprobability of success \n                   0.6 \n\n\nSuppose you are watching a golf tournament on TV and Scottie Scheffler is about to hit an approach shot from the fairway. You hear the announcer say that Scottie Scheffler hits 3/4 of his greens in regulation from the fairway. You are skeptical of this claim and decide to test it against the data you have collected at the 95% confidence level.\nYou set up a hypothesis test with the following hypotheses:\n\nNull Hypothesis \\(H_0\\): The proportion of greens hit in regulation by Scottie Scheffler from the fairway is 0.75.\nAlternative Hypothesis \\(H_A\\): The proportion of greens hit in regulation by Scottie Scheffler from the fairway is not 0.75.\n\n\n\n\n\n\n\nExercise 5: Confidence Intervals for Hypothesis Testing\n\n\n\nUse the binom.test() function to test the hypothesis that the proportion of greens hit in regulation by Scottie Scheffler from the fairway is 0.75.\n\nWhat is the value of the test statistic?\nWhat is the p-value of the test?\nBased on the p-value, do you reject or fail to reject the null hypothesis?",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scheffler Greens in Regulation"
    ]
  },
  {
    "objectID": "golf/pga_gir/index.html#viewing-the-data",
    "href": "golf/pga_gir/index.html#viewing-the-data",
    "title": "PGA - Scheffler Greens in Regulation",
    "section": "Viewing the data",
    "text": "Viewing the data\nWe can use the glimpse() function to get a quick look at our scheffler_2023 data. The glimpse code provides the number of observations (Rows) and the number of variables (Columns) in the dataset. The “Rows” and “Columns” are referred to as the dimensions of the dataset. It also shows us the names of the variables (player, greens, holes …, cut) and the first few observations for each variable.\n\nglimpse(scheffler_2023)\n\nRows: 3\nColumns: 5\n$ player &lt;chr&gt; \"Scottie Scheffler\", \"Scottie Scheffler\", \"Scottie Scheffler\"\n$ year   &lt;dbl&gt; 2023, 2023, 2023\n$ greens &lt;dbl&gt; 247, 657, 28\n$ holes  &lt;dbl&gt; 416, 760, 48\n$ cut    &lt;chr&gt; \"Other\", \"Fairway\", \"Bunker\"\n\n\nAnother useful function to get a quick look at the data is the head() function. This function shows the first few rows of the dataset.\n\nhead(scheffler_2023)\n\n# A tibble: 3 × 5\n  player             year greens holes cut    \n  &lt;chr&gt;             &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  \n1 Scottie Scheffler  2023    247   416 Other  \n2 Scottie Scheffler  2023    657   760 Fairway\n3 Scottie Scheffler  2023     28    48 Bunker \n\n\n\n\n\n\n\n\nExercise 1: Data Structure\n\n\n\n\nWhat are the dimensions of this dataset?\nAre there any variables not useful for our analysis? If so, list the names of these variables.\nWhat type of variable (ex: continuous numeric, catergorical, binary, ordinal…) is the cut variable?\nWhich two variables can be used to calculate the proportion of greens hit in regulation?\n\n\n\n\n\n\nTIP: Type your answers to each exercise in the .qmd document.",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scheffler Greens in Regulation"
    ]
  },
  {
    "objectID": "golf/pga_gir/index.html#testing-hypothesized-proportions",
    "href": "golf/pga_gir/index.html#testing-hypothesized-proportions",
    "title": "PGA - Scheffler Greens in Regulation",
    "section": "Testing Hypothesized Proportions",
    "text": "Testing Hypothesized Proportions\n\nSetting Up Hypotheses\nIn hypothesis testing for proportions, a null hypothesis is set up to test a claim about a population proportion. The null hypothesis is that the population proportion is equal to a specific value. The alternative hypothesis can be that the population proportion is not equal to the specific value, greater than the specific value, or less than the specific value.\nThe options are shown below:\n\n\nNOTE: It is common practice to denote a null hypothesis with \\(H_0\\) and an alternative hypothesis with \\(H_A\\). Sometime the alternative hypothesis is denoted with \\(H_1\\).\n\\(H_0: p = p_0\\)\nand\n\\(H_A: p \\neq p_0\\) or\n\\(H_A: p &gt; p_0\\) or\n\\(H_A: p &lt; p_0\\)\nWhere \\(p_0\\) is the hypothesized value of the population proportion and \\(p\\) is the true population proportion.\n\n\nSignificance Level\nFor hypothesis testing, a significance level is set to determine the probability of rejecting the null hypothesis when it is true. The significance level is denoted by \\(\\alpha\\) and is often set to 0.05. This means that there is a 5% chance of rejecting the null hypothesis when it is actually true. Other common significance levels are 0.01 and 0.10.\n\n\nCalculating the Test Statistic\nA test statistic is a value calculated from the sample data that is used to determine whether the null hypothesis should be rejected or not.\nThe test statistic for hypothesis testing for proportions is:\n\\[z = \\frac{\\hat{p} - p_0}{\\sqrt{\\frac{p_0(1 - p_0)}{n}}}\\]\nWhere \\(\\hat{p}\\) is the sample proportion, \\(p_0\\) is the hypothesized value of the population proportion, and \\(n\\) is the sample size.\n\n\nDetermining the Significance of the Test\nThere are two common ways to determine the significance of the test:\n\nCompare the test statistic to the critical value\nCompare the p-value to the significance level\n\nThe first method compares the test statistic to the critical value. The critical value is the value that separates the rejection region from the non-rejection region. If the test statistic is in the rejection region, the null hypothesis is rejected. This also corresponds to the confidence intervals given for true probability of success. If the hypothesized value of the population proportion is within the confidence interval, the null hypothesis is not rejected. If it is outside the confidence interval, the null hypothesis is rejected.\n\n\nTIP: Critical values can be found very easily in R using the qnorm() function. For example, qnorm(.975) will return the critical value for a 95% confidence interval (two sided test).\nIf the test is right-tailed (greater than), the critical value is found using qnorm(.95) for a 95% confidence interval. If the test is left-tailed (less than), the critical value is found using qnorm(.05) for a 95% confidence interval.\nThe p-value is the most common way to determine the significance of the test. The p-value is the probability of observing a test statistic as extreme as the one calculated from the sample data, assuming the null hypothesis is true. If the p-value is less than the significance level, the null hypothesis is rejected.\n\n\nHypothesis Testing in R\nHypothesis testing for proportions can also be done in R using the prop.test() function. Seen below is an example where it is being tested if the true proportion of successes is different than .7. The first test shows the results if the sample proportion had 60 out of 100 successes and the second test shows the results if the sample proportion had 70 out of 100 successes. Both tests are done using an \\(\\alpha\\) level of 0.05.\n\nprop.test(60, 100, p = .7, conf.level = .95, correct = FALSE)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  60 out of 100, null probability 0.7\nX-squared = 4.7619, df = 1, p-value = 0.0291\nalternative hypothesis: true p is not equal to 0.7\n95 percent confidence interval:\n 0.5020026 0.6905987\nsample estimates:\n  p \n0.6 \n\nprop.test(70, 100, p = .7, conf.level = .95, correct = FALSE)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  70 out of 100, null probability 0.7\nX-squared = 4.2073e-31, df = 1, p-value = 1\nalternative hypothesis: true p is not equal to 0.7\n95 percent confidence interval:\n 0.6041515 0.7810511\nsample estimates:\n  p \n0.7 \n\n\nThe test statistic is returned in the output of prop.test() as X-squared. This is the square of the z-value. For the first test the test statistic is 2.182, and for the second test the test statistic is 0. For a 95% confidence level for a two-sided test, the critical value is 1.96. If the test statistic is greater than 1.96 or less than -1.96, the null hypothesis is rejected. Since the test statistic for the first test is less than -1.96, the null hypothesis is rejected. Since the test statistic for the second test is between -1.96 and 1.96, the null hypothesis is not rejected.\nThe p-value of the test is returned in the output of the prop.test() function. For the first test, the p-value is 0.029 and for the second test, the p-value is 1. Since the p-value of the first test is less than the significance level of 0.05, the null hypothesis is rejected. Since the p-value for the second test is greater than the significance level of 0.05, we fail to reject the null hypothesis.\n\n\nDrawing Conclusions\nIf the null hypothesis is rejected we can conclude that the sample data provides enough evidence to support the alternative hypothesis. If the null hypothesis was that the population proportion is equal to .7 and the alternative hypothesis was that the population proportion is not equal to .7 and \\(\\alpha = 0.5\\), then we might say,\n“There is significant evidence to suggest that the true population proportion is not equal to .7 at the 95% confidence level.”\nIf the null hypothesis is not rejected we do not automatically accept the null hypothesis. We simply do not have enough evidence to reject it. For example if the null hypothesis was that the population proportion is equal to .7 and the alternative hypothesis was that the population proportion is not equal to .7 and \\(\\alpha = .05\\), then we might say,\nThere is not enough evidence to suggest that the true population proportion is different from .7 at the 95% confidence level.”\n\n\nExample\nSuppose you are watching a golf tournament on TV and Scottie Scheffler is about to hit an approach shot from the fairway. You hear the announcer say that Scottie Scheffler hits 3/4 of his greens in regulation from the fairway. You are skeptical of this claim and decide to test it against the data you have collected at the 95% confidence level.\nYou set up a hypothesis test with the following hypotheses:\n\nNull Hypothesis \\(H_0\\): The proportion of greens hit in regulation by Scottie Scheffler from the fairway is 0.75.\nAlternative Hypothesis \\(H_A\\): The proportion of greens hit in regulation by Scottie Scheffler from the fairway is not 0.75.\n\n\n\n\n\n\n\nExercise 5: Confidence Intervals for Hypothesis Testing\n\n\n\nUse the prop.test() function to test the hypothesis that the true proportion of greens hit in regulation by Scottie Scheffler from the fairway is not 0.75. \\(\\alpha = 0.05\\). Set correct = FALSE.\n\nWhat is the p-value of the test?\nBased on the p-value, do you reject or fail to reject the null hypothesis?\nWhat is your conclusion?",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scheffler Greens in Regulation"
    ]
  },
  {
    "objectID": "golf/pga_masters/index.html",
    "href": "golf/pga_masters/index.html",
    "title": "PGA - Scoring Average Confidence Intervals",
    "section": "",
    "text": "Facilitation notes\n\n\n\n\n\n\nThis module would be suitable for an in-class lab or take-home assignment in an introductory statistics course.\nIt assumes a basic familiarity with the RStudio Environment and R programming language.\nStudents should be provided with the following data file (.csv) and Quarto document (.qmd) to produce visualizations and write up their answers to each exercise. Their final deliverable is to turn in an .html document produced by “Rendering” the .qmd.\n\ndata\nStudent Quarto template\n\nPosit Cloud (via an Instructor account) or Github classroom are good options for disseminating files to students, but simply uploading files to your university’s course management system works, too.",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scoring Average Confidence Intervals"
    ]
  },
  {
    "objectID": "golf/pga_masters/index.html#terms-to-know",
    "href": "golf/pga_masters/index.html#terms-to-know",
    "title": "PGA - Scoring Average Confidence Intervals",
    "section": "Terms to know",
    "text": "Terms to know\nBefore proceeding with the analysis, let’s make sure we know some important golf terminology that will help us master this lab.\n\nGolf Terminology\n\nPar in golf is the amount of strokes that a good golfer is expected to take to get the ball in the hole.\n\nEach hole in golf has its own par. There are par 3 holes, par 4 holes, and par 5 holes.\nThere are 18 holes on a golf course and the pars of each of these holes sums to par for the course, also known the course par.\n\nA round in golf is when a golfer plays the full set of 18 holes on the course.\n\nIn most professional golf tournaments, all golfers play 2 rounds, the best golfers are selected and those golfers play 2 more rounds for a total of 4 rounds.\n\n\n\n\n\n\n\n\nTypes of Golf Tours\n\n\n\n\nIn golf there are a few tours, better thought of as leagues, that golfers regularly compete in\n\nThe PGA, or “Professional Golf Association”, has long been considered the preeminant golfing tour, hosting most tournaments and containing the most skilled members.\nThe LIV tour is a Saudi-backed alternative to the PGA that began playing tournaments in 2022. LIV is the Roman numeral for 54 and is related to the fact that LIV tournaments only allow 54 players and only play 54 holes, compared to the normal PGA 72 holes.\nThe PGA Tour Champions is a branch off of the PGA tour for players 50 or older. It used to be called the “Senior PGA Tour” until 2003, when it began being called the Champions tour\nAn amateur is a golfer who is not yet a professional. They are not allowed to win money in professional golf tournaments. Most amateurs are college golfers.\n\n\n\n\n\n\nPGA vs. LIV\n\nvs\n\nImages Source: PGA and LIV\nClick here to read about LIV golf’s founding and its continued impact on the PGA tour.",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scoring Average Confidence Intervals"
    ]
  },
  {
    "objectID": "golf/pga_masters/index.html#variable-descriptions",
    "href": "golf/pga_masters/index.html#variable-descriptions",
    "title": "PGA - Scoring Average Confidence Intervals",
    "section": "Variable descriptions",
    "text": "Variable descriptions\nThe masters_2023 data you’ll be analyzing in this lab provides scores for each round by each golfer in the 2023 Masters. The data includes the names of golfers, the round, their scores, and their tour.\n\n\nVariable Descriptions\n\n\n\n\nVariable\nDescription\n\n\n\n\nplayer\nGolfer’s name\n\n\nround\nRound of the tournament\n\n\nscore\nScore for the 18-hole course\n\n\ntour\nThe tour the player generally competes on",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scoring Average Confidence Intervals"
    ]
  },
  {
    "objectID": "golf/pga_masters/index.html#t-interval-for-single-means",
    "href": "golf/pga_masters/index.html#t-interval-for-single-means",
    "title": "PGA - Scoring Average Confidence Intervals",
    "section": "t-interval for single means",
    "text": "t-interval for single means\nA t-distribution is used for calculating a single mean confidence interval if the sample size is small (rule of thumb: less than 30) and the population standard deviation is unknown.\nThe formula for calculating a CI using this method is shown below: \\[CI = \\bar{X} \\pm t_{\\alpha/2, df} \\times \\frac{S}{\\sqrt{n}}\\]\n\n\nClick here to learn more about the t-distribution and play around with t-distribution graphs.\nWhere \\(\\bar{X}\\) is the sample mean,\n\\(t_{\\alpha/2, df}\\) is the critical value for the t-distribution with \\(df = n-1\\),\n\\(S\\) is the sample standard deviation,\nand \\(n\\) is the sample size.\n\n\nNOTE: The t-distribution changes based on the degrees of freedom, approaching the normal distribution as the degrees of freedom increase.\nThe critical value for the t-distribution is determined by the confidence level and the degrees of freedom. This is calculated by finding the value of \\(t_{\\alpha/2, df}\\) such that the area under the t-distribution curve (with that specific degrees of freedom) between \\(-t_{\\alpha/2, df}\\) and \\(t_{\\alpha/2, df}\\) is equal to the confidence level.\n\n\n\n\n\n\n\n\n\n\n\nNOTE: Using R to calculate the critical value for the t-distribution saves time and adds accuracy compared to using a t-table.\nR can easily calculate the critical value for the t-distribution using the qt() function.\nThe qt() function takes two arguments: the first is the confidence level, and the second is the degrees of freedom. The example below shows how to calculate the t-value for a 95% confidence level with 10 degrees of freedom.\nNote that the 95% confidence interval has \\(\\alpha = .05\\) and \\(\\alpha / 2 =  .025\\) so we need to use qt(.975, df= 10) or qt(.025, df = 10) to find the critical values.\n\nqt(0.975, df = 10)\n\n[1] 2.228139\n\n\n\n\n\n\n\n\nExercise 2: t-distribution confidence intervals\n\n\n\n\nRun the code below to create the proper subset of the data for this exercise and calculate the sample mean and sample standard deviation.\n\namateurs_round1 &lt;- masters_2023 |&gt; \n  filter(round == 1, tour == \"Amateur\")\nx_bar &lt;- amateurs_round1 |&gt; \n  summarise(mean(score)) |&gt; \n  pull()\nstd_dev &lt;- amateurs_round1 |&gt; \n  summarize(sd(score)) |&gt; \n  pull()\nn &lt;- nrow(amateurs_round1)\nt_cv &lt;- qt(0.95, df = n - 1)\n\nUse the formula for creating a confidence interval using the t-distribution to calculate the upper and lower limits of a confidence interval for the true mean of amateur scoring using the amateurs in the first round as our sample. Use a 90% confidence interval (\\(\\alpha = .1\\)).\n\nWhat is the lower bound of the confidence interval?\nWhat is the upper bound of the confidence interval?\nWhat is the interpretation of this confidence interval?\n\n\n\nRepeat this process with a 99% confidence interval (\\(\\alpha = .01\\)).\n\nWhat is the new lower bound of the confidence interval?\nWhat is the new upper bound of the confidence interval?\nIs this 99% confidence interval larger, smaller, or the same as 90% confidence interval?\nWhen thinking about your anwer to f, what do you think could explain this?\n\n\n\nTIP: Values and objects can be stored in variables in R. For example, x &lt;- 5 stores the value 5 in the variable x.\nTIP: The pipe operator is a powerful tool in R that allows you to chain functions together. It is denoted by |&gt; and is used to pass the output of one function to the input of another function.\nTIP: The pull() function is used to extract a single column from a data frame as a vector.\nTIP: The nrow() function is used to calculate the number of rows in a data frame.\n\nTIP: When interpreting a confidence interval do not say “there is a 90% chance that the true mean is between the lower and upper bounds”. Instead, say “we are 90% confident that the true mean is between the lower and upper bounds”.\n\nTIP: Only the t_cv variable needs to be changed to recompute with the new confidence interval. Use qt(0.995, df = n - 1) to calculate the critical value for the new 99% confidence interval.",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scoring Average Confidence Intervals"
    ]
  },
  {
    "objectID": "golf/pga_masters/index.html#z-interval-for-single-means",
    "href": "golf/pga_masters/index.html#z-interval-for-single-means",
    "title": "PGA - Scoring Average Confidence Intervals",
    "section": "z-interval for single means",
    "text": "z-interval for single means\nA standard normal distribution (also known as a z-distribution) is used to calculate the confidence interval for a single mean if the sample size is large enough (greater than 30) or the population standard deviation is known. The first case is common as oftentimes samples are greater than 30. The second case is rare because it is uncommon to know the population standard deviation but not the population mean.\nThe formula for the confidence interval for a single mean using the z-distribution is very similar to that of the t-distribution\n\n\nClick here for more information about the standard normal distribution.\n\\(CI = \\bar{X} \\pm Z_{\\alpha/2} \\times \\frac{\\sigma}{\\sqrt{n}}\\)\nWhere \\(\\bar{X}\\) is once again the sample mean, \\(Z_{\\alpha/2}\\) is the critical value for the standard normal distribution at the specified confidence level, \\({\\sigma}\\) is the population standard deviation, and \\(n\\) is the sample size.\nThe reasoning behind why we can use the standard normal distribution when the sample is greater than 30 even if the population standard deviation is unknown is found in the Central Limit Theorem, which says that as the sample size increases, the sampling distribution of the sample mean approaches a normal distribution. This means that when the sample size is greater than 30 we can use the sample standard deviation to estimate the population standard deviation and create a confidence interval as seen below\n\\(CI = \\bar{X} \\pm Z_{\\alpha/2} \\times \\frac{S}{\\sqrt{n}}\\)\nOnce again the critical value for the z-distribution is the value of \\(Z_{\\alpha/2}\\) such that the area under the standard normal distribution curve between \\(-Z_{\\alpha/2}\\) and \\(Z_{\\alpha/2}\\) is equal to the confidence level.\nR can compute the z-value for you using the qnorm() function. The qnorm() function takes in the probability and returns the z-value that corresponds to that probability. For example, qnorm(.975) will return the z-value that corresponds to the 97.5th percentile of the standard normal distribution. No degrees of freedom are needed.\nThe code below calculates the critical values for the z-distribution for a 95% confidence interval.\n(Note that .975 is used for a 95% confidence interval because \\(\\alpha = .05\\) and since the confidence interval is two sided we need half of the error each side so \\(\\alpha / 2  = .025\\), which means we want to use qnorm(.975) or qnorm(.025))\n\nqnorm(.975)\n\n[1] 1.959964\n\nqnorm(.025)\n\n[1] -1.959964\n\n\n\n\n\n\n\n\nExercise 3: z-distribution confidence intervals\n\n\n\nRun the code below to create the proper subset of the data for this exercise and calculate the sample mean, sample standard deviation, and critical value for the z-distribution.\n\npga_round1 &lt;- masters_2023 |&gt; \n  filter(round == 1, tour == \"PGA\")\nx_bar &lt;- pga_round1 |&gt; \n  summarise(mean(score)) |&gt; \n  pull()\nsigma &lt;- pga_round1 |&gt; \n  summarise(sd(score)) |&gt; \n  pull()\nz_cv &lt;- qnorm(.975)\n\nUse the formula for creating a confidence interval using the standard normal distribution to calculate the upper and lower limits of a confidence interval for the true mean of PGA professional scoring at Augusta using the PGA pros in the first round as our sample. Use a 95% confidence interval (\\(\\alpha = .05\\)).\n\nWhy can we use the standard normal distribution to calculate this confidence interval?\nWhat is the confidence interval for the true mean of scoring for PGA professionals at Augusta National?\nWhat is the interpretation of this confidence interval?",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scoring Average Confidence Intervals"
    ]
  },
  {
    "objectID": "golf/pga_masters/index.html#r-for-single-mean-confidence-intervals",
    "href": "golf/pga_masters/index.html#r-for-single-mean-confidence-intervals",
    "title": "PGA - Scoring Average Confidence Intervals",
    "section": "R for Single Mean Confidence Intervals",
    "text": "R for Single Mean Confidence Intervals\nR functions can help to speed up the process of finding these confidence intervals and will also help us with testing hypotheses later. The t.test function from the stats package makes a confidence interval for the t-distribution and the z.test function from the BSDA package does the same for the standard normal distribution.\nRun the code below to view what type of arguments these functions take, what they output, and the see some example uses\n\n?t.test\n?z.test\n\nThese functions return more than just a confidence interval. If we want to see just the confidence interval $conf.int should be used. Run the example below to see how this is done.\n\n# t-test example\nseniors_round1 &lt;- masters_2023 |&gt; \n  filter(round == 1, tour == \"Senior\")\n\nt.test(seniors_round1$score, conf.level = .95)$conf.int\n\n[1] 72.39194 79.03663\nattr(,\"conf.level\")\n[1] 0.95\n\n# z-test example\npga_round2 &lt;- masters_2023 |&gt; \n  filter(round == 2, tour == \"PGA\")\n\n## Note that the Z test require the standard deviation to be passed in as an argument `sigma.x`\nz.test(pga_round2$score,\n       sigma.x = sd(pga_round2$score),\n       conf.level = .95)$conf.int\n\n[1] 72.24391 73.71973\nattr(,\"conf.level\")\n[1] 0.95\n\n\n\n\nTIP: The $ operator is used to access a specific element of a list. In this case, the conf.int element of the list returned by the t.test and z.test functions. It can also be used to access elements of data frames and other objects in R. The line of code seniors_round1$score is used to access the score column of the seniors_round1 data frame.\n\nTIP: Notice that the z.test function requires the standard deviation to be passed in as an argument sigma.x. Use the sd() function to calculate the standard deviation of the sample and use it as an estimate the population standard deviation.\n\n\n\n\n\n\n\nExercise 4: R for confidence intervals\n\n\n\n\nUse the t.test function to find the confidence interval for amateur scoring using the amateur_round1 sample from earlier in the lesson. What is the confidence interval?\n\n\n\nUse the z.test function to find the confidence interval for PGA professional scoring using the pga_round1 sample from earlier in the lesson. What is the confidence interval?\nAre these confidence intervals the same as what you calculated manually earlier? If no, why might that be?\n\n\n\n\nTIP If you didn’t create the amateur_round1 sample earlier run the code below\n\namateur_round1 &lt;- masters_2023 |&gt; \n  filter(round == 1, tour == \"Amateur\")\n\nTIP If you didn’t create the pga_round1 sample earlier run the code below\n\npga_round1 &lt;- masters_2023 |&gt; \n  filter(round == 1, tour == \"PGA\")",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scoring Average Confidence Intervals"
    ]
  },
  {
    "objectID": "golf/pga_masters/index.html#test-statistics",
    "href": "golf/pga_masters/index.html#test-statistics",
    "title": "PGA - Scoring Average Confidence Intervals",
    "section": "Test Statistics",
    "text": "Test Statistics\nLike confidence intervals, we have two different tests for hypothesis testing for the population mean. Remember that if the population standard deviation is unknown and the sample size is less than 30, we use the t-distribution. If the population standard deviation is known or the sample size is greater than 30, we use the standard normal distribution.\nEach of these distributions have their own tests, the t-test and the z-test. This means that we have different test statistics to calculate depending on the situation.\n\nt-test\nThe t-test statistic is calculated using the formula:\n\\[t = \\frac{\\bar{x} - \\mu_0}{\\frac{s}{\\sqrt{n}}}\\]\nwhere \\(\\bar{x}\\) is the sample mean, \\(\\mu_0\\) is the hypothesized population mean, \\(s\\) is the sample standard deviation, and \\(n\\) is the sample size.\n\n\nz-test\nThe z-test statistic is calculated using the formula:\n\\[z = \\frac{\\bar{x} - \\mu_0}{\\frac{\\sigma}{\\sqrt{n}}}\\]\nwhere \\(\\bar{x}\\) is the sample mean, \\(\\mu_0\\) is the hypothesized population mean, \\(\\sigma\\) is the population standard deviation, and \\(n\\) is the sample size.",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scoring Average Confidence Intervals"
    ]
  },
  {
    "objectID": "golf/pga_masters/index.html#to-reject-or-fail-to-reject",
    "href": "golf/pga_masters/index.html#to-reject-or-fail-to-reject",
    "title": "PGA - Scoring Average Confidence Intervals",
    "section": "To Reject or Fail to Reject",
    "text": "To Reject or Fail to Reject\nThere are two ways to make a decision about the null hypothesis.\nMethod 1: Critical values, along with test statistics, can be used to determine if the hypothesized population mean is within the confidence interval for the true mean.\nA critical value is a value that separates the rejection region from the non-rejection region. The rejection region is the area where the null hypothesis is rejected. The non-rejection region is the area where the null hypothesis is not rejected. The critical value is determined by the significance level (\\(\\alpha\\)) and the degrees of freedom (if it is a t-test). The critical value is compared to the test statistic to determine if the null hypothesis should be rejected. If the test statistic is within the non-rejection region, the null hypothesis is not rejected. If the test statistic is within the rejection region, the null hypothesis is rejected and the alternative hypothesis is accepted.\nBelow is an example of using critical values and a test-statistic for a z-test with a 95% confidence level (two-sided). The critical value is 1.96. This means that if the test statistic is greater than 1.96 or less than -1.96, the null hypothesis is rejected. The blue represents the non-rejection region and the red the rejection region. Since the Test Statistic for this hypothetical example is 1.1 (less than 1.96 and greater than -1.96), we fail to reject the null hypothesis.\n\n\n\n\n\n\n\n\n\nThis method corresponds directly to the related confidence intervals produced for the sample data.\nIf the hypothesized population mean is within the confidence interval, we fail to reject the null hypothesis. If the hypothesized population mean is not within the confidence interval, the null hypothesis is rejected and the alternative hypothesis is accepted.\n\n\nNote: We can say that there is significant evidence to accept the alternative hypothesis if the null hypothesis is rejected. However, it should never be said that we accept the null hypothesis. We can only fail to reject it.\nMethod 2: The second method is to use a p-value. The p-value is the probability of observing a test statistic as extreme as the one calculated from the sample data given that the null hypothesis is true. The p-value is compared to the significance level (\\(\\alpha\\)) to determine if the null hypothesis should be rejected. If the p-value is less than \\(\\alpha\\), the null hypothesis is rejected. If the p-value is greater than \\(\\alpha\\), the null hypothesis is not rejected.\nFor example, if the p-value for a single mean hypothesis test is 0.03 and the significance level is 0.05, the null hypothesis is rejected because the p-value is less than the significance level.",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scoring Average Confidence Intervals"
    ]
  },
  {
    "objectID": "golf/pga_masters/index.html#hypothesis-testing-in-r",
    "href": "golf/pga_masters/index.html#hypothesis-testing-in-r",
    "title": "PGA - Scoring Average Confidence Intervals",
    "section": "Hypothesis testing in R",
    "text": "Hypothesis testing in R\nThankfully R can help us with this as well. The t.test function and the z.test function can both perform hypothesis tests.\nThe code below tests if the true mean is not equal to 50 given that the example_vector is our sample. The confidence level is set to 95%.\n\nexample_vector &lt;- seq(10, 100, by = 10)\nt.test(example_vector, mu = 50, alternative = \"two.sided\", conf.level = .95)\n\n\n    One Sample t-test\n\ndata:  example_vector\nt = 0.52223, df = 9, p-value = 0.6141\nalternative hypothesis: true mean is not equal to 50\n95 percent confidence interval:\n 33.34149 76.65851\nsample estimates:\nmean of x \n       55 \n\n\nAs can be seen in the output of the code above, the p-value is 0.61, which is much greater than the significance level of 0.05. The test-statistic is 0.522 which is within the non-rejection region since critical values for this test would be -2.26 and 2.26. Therefore, we fail to reject the null hypothesis.",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scoring Average Confidence Intervals"
    ]
  },
  {
    "objectID": "golf/pga_masters/index.html#hypothesizing-par-as-the-population-mean",
    "href": "golf/pga_masters/index.html#hypothesizing-par-as-the-population-mean",
    "title": "PGA - Scoring Average Confidence Intervals",
    "section": "Hypothesizing Par as the Population Mean",
    "text": "Hypothesizing Par as the Population Mean\n\n\nAugusta National is breathtakingly beautiful, but if golfers get distracted by the scenic views, tall pines, bunkers, water, and azaleas may catch their balls.\nThe 13th hole at Augusta National \nImage Source: Golf.com\nIn golf par is considered to be the number of strokes a good golfer is expected to take. The par for the course at Augusta National is 72. It is known that Augusta National is a tougher than usual course but we would like to test if that is the case for different groups.\nOur null hypothesis will generally be that the mean of the group is equal to 72.\n\n\n\n\n\n\nExercise 5: t-test for single mean hypothesis testing\n\n\n\n\nAmateurs, who are not yet professional golfers, are generally expected to score higher than professionals. We would like to test if the mean of amateur scoring is above par at Augusta National\n\nWhat is the null hypothesis for this test?\nWhat is the alternative hypothesis for this test?\nUse the t.test function to test if the mean of amateur scoring is greater than 72 using the amateur_round1 sample from earlier in the lesson. What is the p-value?\nBased on the p-value, is there statistically significant evidence that the mean of amateur scoring is greater than 72?\n\n\n\n\nAmateurs generally struggle in the Masters, but in 2023 Sam Bennett, a Texas A&M student, made the cut and finished 16th. However, due to his amateur status, he was not eligible to win money and missed out on $261,000.\nSam Bennett \nImage Source: Sportico\n\n\n\n\n\n\nExercise 6: z-test for single mean hypothesis testing\n\n\n\nPGA professionals would generally average somewhere around par. We would like to test if the mean of PGA professional scoring is not equal to 72 at Augusta National.\n\nWhat is the null hypothesis for this test?\nWhat is the alternative hypothesis for this test?\n\n\n\nUse the z.test function to test if the mean of PGA professional scoring is not equal to 72 using the pga_round1 sample from earlier in the lesson. What is the p-value?\nBased on the confidence interval, is there statistically significant evidence that the mean of PGA professional scoring is not equal to 72?\nExplain your answer to part d.\n\n\n\n\nTIP Use the $score column from the pga_round1 sample as the first argument in the z.test function\nTIP Remember that the z.test function requires the population standard deviation as the second argument. Use the sd function to calculate the standard deviation of the pga_round1 sample.\n\nsd(pga_round1$score)",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scoring Average Confidence Intervals"
    ]
  },
  {
    "objectID": "mma/mma_chisquare/index.html",
    "href": "mma/mma_chisquare/index.html",
    "title": "MMA Fight Decisions",
    "section": "",
    "text": "Facilitation notes\n\n\n\n\n\n\nThis module would be suitable for an in-class lab or take-home assignment in an introductory statistics course.\nIt assumes a basic familiarity with the RStudio Environment has already been covered, but no prior programming experiences is expected.\nStudents should be provided with the following data file (.csv) and Quarto document (.qmd) to produce tests/visualizations and write up their answers to each exercise. Their final deliverable is to turn in an .html document produced by “Rendering” the .qmd.\n\ndata\nStudent Quarto template\n\nPosit Cloud (via an Instructor account) or Github classroom are good options for disseminating files to students, but simply uploading files to your university’s course management system works, too.",
    "crumbs": [
      "Home",
      "Mma",
      "MMA Fight Decisions"
    ]
  },
  {
    "objectID": "mma/mma_chisquare/index.html#mma-lingo",
    "href": "mma/mma_chisquare/index.html#mma-lingo",
    "title": "MMA Fight Decisions",
    "section": "MMA lingo",
    "text": "MMA lingo\nBefore proceeding with any analysis, let’s make sure we know some MMA lingo in order to understand what information is contained in each variable (column) in our dataset.\n\nThe basics\n\nFighters are in different weight classes and may only fight in their own class or higher weight classes.\n\nThere are 8 classes for men and 4 for women.\n\nFights consist of rounds (typically 3).\nFights may end early due to:\n\nA submission: one fighter “taps out” or concedes.\nKnock out (KO) or Technical knock out (TKO) (the referee stops the fight).\n\nIf a fight is not stopped early, then the winner is determined by judges (known as a decision).\n\nThe decision could be unanimous (U) if all judges agree, or split (S) if not all judges agree.\n\nChampionship fights are those which determine who is the champion of a given weight class.\n\n\n\nTotally new to MMA? See this site: INTRODUCTION TO MMA\n\n\n\n\n\n\nWeight Classes in the mma data set\n\n\n\n\nCurrently there are 12 UFC weight classes. From lightest to heaviest:\n\nStrawweight 115 lb, Flyweight 125 lb, Bantamweight 135 lb, Featherweight 145 lb, Lightweight 155 lb, Super lightweight 165 lb, Welterweight 170 lb, Super welterweight 175 lb, Middleweight 185 lb, Super middleweight 195 lb, Light heavyweight 205 lb, Cruiserweight 225 lb, Heavyweight 265 lb, Super heavyweight\n\n\nOur mma data set includes fights from only 9 of the 12 classes.\nThe original data listed both fighter weight classes\n\nFor some fights the fighters were from different weight classes\nWe assumed the fight weight class was the heavier class of the two fighters.\n\nThree additional weight classes are listed in the data:\n\nAtomweight: a class recognized by some fight organizations for women that is lighter (105 lbs) than the 12 classes.\nCatchweight: fights that don’t follow traditional weight classes, often agree to in contracts between fighters.\nOpen Weight: unofficial weight class that allow fighters of different sizes to compete against each other with no weight limit.",
    "crumbs": [
      "Home",
      "Mma",
      "MMA Fight Decisions"
    ]
  },
  {
    "objectID": "mma/mma_chisquare/index.html#variable-descriptions",
    "href": "mma/mma_chisquare/index.html#variable-descriptions",
    "title": "MMA Fight Decisions",
    "section": "Variable descriptions",
    "text": "Variable descriptions\nThe data used for the following examples is MMA fight data. The data is a SAMPLE of fights from 1991 to 2023 (not all fights in that period). In addition to data about the fight generally (date, name of event, whether it was a “championship” fight etc) data is provided for each fighter participating in the fight (columns starting with p1 are the first fighter and p2 the second).\n\n\n\n\n\n\n\n\n\n\nVariable\nDefinition\nExample_Values\n\n\n\n\ndate\ndate of the event (mdy)\n10/12/2023\n\n\nmonth\nmonth of the event\n10\n\n\nyear\nyear of the event\n2023\n\n\nevent\nname of the event\nUFC 1: The Beginning\n\n\nchampionship\nis the event a championship event\nTRUE/FALSE\n\n\ndecision\nfight result\nTKO (Injury)\n\n\ndecision_group\naggregated groups of decision variable\nKO/TKO\n\n\nround\nhow many rounds the fight lasted\n3\n\n\ntime\ntime the fight ended\n4:42\n\n\np1_result\nresult of the fight for fighter 1(2)\nW\n\n\np1_id\na unique ID for fighter 1(2)\n2354059\n\n\np1_name\nfull name of fighter 1(2)\nShamrock, Ken\n\n\np1_country\nhome country of fighter 1(2)\nUSA\n\n\np1_sex\nsex of fighter 1(2)\nM\n\n\nwtClass\naggregated groups of Weight Class\nHeavyweight",
    "crumbs": [
      "Home",
      "Mma",
      "MMA Fight Decisions"
    ]
  },
  {
    "objectID": "mma/mma_chisquare/index.html#viewing-your-data",
    "href": "mma/mma_chisquare/index.html#viewing-your-data",
    "title": "MMA Fight Decisions",
    "section": "Viewing your data",
    "text": "Viewing your data\nYou saw that glimpse() is one way to get a quick look at your data. Often, you’ll want to view your whole dataset. There are two ways to do this:\n\n\n\nTIP: Recall that RStudio is split into four quadrants: Source (upper left), Environment (upper right), Console (bottom left), and Files/Plots/Packages/Help/Viewer (bottom right)\n\ntype View(mma) in your Console and then click return/Enter on your keyboard.\nOR, in your Environment tab, double click the name of the dataset you want to view.\n\nThis will open up your data in a new viewer tab so that you can view it like a spreadsheet (like Google Sheets or Excel*). Once open, you can sort the data by clicking on a column.\n\n\n*Unlike Google Sheets or Excel, however, you won’t be able to edit the data directly in the spreadsheet.\n\n\n\n\n\n\nExercise 2\n\n\n\nView the mma data and sort it appropriately to answer the following questions:\n\nWhat was the latest fight date in the data set?\nWhat was the event of the fight(s) on this date?\nHow many fights took place at this event?\nHow many of these fights ended with a KO or TKO?\nWhich fighters one the fights ending in KO or TKO?\nWhat weight class were the fighters who won by KO or TKO?\n\n\n\n\n\nTIP: When viewing the data, clicking on a column once will sort the data according to that variable in ascending order; clicking twice will sort in descending order.",
    "crumbs": [
      "Home",
      "Mma",
      "MMA Fight Decisions"
    ]
  },
  {
    "objectID": "mma/mma_chisquare/index.html#preparing-the-data",
    "href": "mma/mma_chisquare/index.html#preparing-the-data",
    "title": "MMA Fight Decisions",
    "section": "Preparing the data",
    "text": "Preparing the data\nWe first select the variables of interest, and “filter” the data to the categories we wish to compare and the years of interest.\n\n# Select, filter, and prepare relevant data for analysis\ndata1 &lt;- mma |&gt; \n  select(year, wtClass, decision_group) |&gt; \n  filter(year &gt;= 2020) |&gt;\n  filter(wtClass == \"Middleweight\" | wtClass == \"Heavyweight\") |&gt; \n  mutate(decision = if_else(decision_group == \"Decision\", \"Decision\",\n                            \"KO/TKO/Submission\"))\n\n\n\nTIP: in logical expressions, a double equal sign is used “==”.\n\nTIP: in logical expressions, “|” is “OR”\n\nTIP: mutate is a dplyr function that allows us to create new variables.\n\nTIP: if_else returns the first value listed (“Decision”) if the logical expression is true, otherwise it returns the second value listed (“KO/TKO/Submission”)\n\n\n\nView the “data1” data frame and confirm that you understand what the code produced.",
    "crumbs": [
      "Home",
      "Mma",
      "MMA Fight Decisions"
    ]
  },
  {
    "objectID": "mma/mma_chisquare/index.html#visualizing-the-data-the-contingency-table",
    "href": "mma/mma_chisquare/index.html#visualizing-the-data-the-contingency-table",
    "title": "MMA Fight Decisions",
    "section": "Visualizing the data: the contingency table",
    "text": "Visualizing the data: the contingency table\nA contingency table rows (r) are categories of the first variable and columns (c) those of the second variable. The “r x c” table cells are the counts for each combination of categories. In our example, both variables have two categories so we have a “2x2” contingency table:\n\ntable(data1$wtClass, data1$decision )\n\n              \n               Decision KO/TKO/Submission\n  Heavyweight       241               570\n  Middleweight      164               334\n\n\n\n\nTIP: select the variable from “data1” using a “$” followed by the name of the variable.\nWe see that among the heavyweight fights, 241 ended with a decision.\nThe R package “DescTools” we loaded includes a function that will add percentages (by row, column, or total) as well as the totals for rows/columns (known as “margins”) to the table.\n\nPercTable(data1$wtClass, data1$decision, rfrq = \"010\",\n                     margins = c(1,2))\n\n                                                         \n                       Decision   KO/TKO/Submission   Sum\n                                                         \nHeavyweight    freq         241                 570   811\n               p.row      29.7%               70.3%     .\n                                                         \nMiddleweight   freq         164                 334   498\n               p.row      32.9%               67.1%     .\n                                                         \nSum            freq         405                 904 1'309\n               p.row      30.9%               69.1%     .\n                                                         \n\n\n\n\nTIP: the rfrq option identifies which percentages to display. A “1” in the second position as in our example adds row percentages. Changing the first position to “1” would give total percentages and to the third position column percentages.\n\nTIP: we provide a vector, c(), with values 1 (row) and 2 (column) of which margins to add to the table. If we input c(1) we would get only the row totals.\n\nWe added row percentages; notice that the percentages total to 100% for each row of the table, including for the column totals row.\n\n\n\n\n\n\nExercise 3\n\n\n\nUse the contingency tables to answer the following questions:\n\nHow many middleweight fights are included in the sample?\nHow many middleweight fights ended with a KO/TKO/Submission? What percentage of middleweight fights in the sample does this represent?\nWhich weight class had a higher percentage of fights ending in KO/TKO/Submission?\nBased on the contingency table, does there appear to be a meaningful association between weight class and how the fight ended? In other words, is the proportion of fights ending in decision a lot different in the two weight classes?",
    "crumbs": [
      "Home",
      "Mma",
      "MMA Fight Decisions"
    ]
  },
  {
    "objectID": "mma/mma_chisquare/index.html#visualizing-the-data-mosaic-plot",
    "href": "mma/mma_chisquare/index.html#visualizing-the-data-mosaic-plot",
    "title": "MMA Fight Decisions",
    "section": "Visualizing the data: Mosaic plot",
    "text": "Visualizing the data: Mosaic plot\nA Mosaic plot visualizes the contingency table using rectangles with widths proportional to the counts for each category or combination of categories.\n\nlibrary(ggmosaic)\nggplot(data1) + \n  geom_mosaic(aes(x = product(wtClass, decision), fill = wtClass)) +\n  xlab('Decision')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\nUse the Mosaic plot to answer the following questions (use the contingency table to assist!):\n\nAre there more fights ending in decision or in KO/TKO/Submission in the data set (how do you know this from the Mosaic plot)?\nWhich weight class appears more likely to have a KO/TKO/Submission? Does this seem to be a big difference between the weight classes?",
    "crumbs": [
      "Home",
      "Mma",
      "MMA Fight Decisions"
    ]
  },
  {
    "objectID": "mma/mma_chisquare/index.html#conducting-the-chi-square-test",
    "href": "mma/mma_chisquare/index.html#conducting-the-chi-square-test",
    "title": "MMA Fight Decisions",
    "section": "Conducting the Chi-square test",
    "text": "Conducting the Chi-square test\nWe first write the hypotheses presented earlier for our example:\n\n\\(H_0:\\) there is no assocation between weight class and whether the fight ended in with a decision or not (null hypothesis)\n\\(H_a:\\) there is an assocation between weight class and whether the fight ended in with a decision or not (alternative hypothesis)\n\nThe command “chisq.test” with the two variables as inputs performs the Chi-square test.\n\nwt_decision.chisq &lt;- chisq.test(data1$wtClass, data1$decision)\n\n\n\nTIP: save the test results into an object (here named “wt_decision.chisq”) in order to later extract all information produced.\n\nObserved counts\nWe first confirm that the test has the correct data by extracting the “observed” values. These should be the same as in the contingency table.\n\nwt_decision.chisq$observed\n\n              data1$decision\ndata1$wtClass  Decision KO/TKO/Submission\n  Heavyweight       241               570\n  Middleweight      164               334\n\n\n\n\nExpected counts\nWe next extract the “expected” values computed in order to perform the test. These are values that would be observed if the sample perfectly matched the proportions suggested in the null hypothesis - exactly the same proportions in the two decision categories for each weight class category (and vice versa). You will explore this in the next exercise.\n\nwt_decision.chisq$expected\n\n              data1$decision\ndata1$wtClass  Decision KO/TKO/Submission\n  Heavyweight  250.9206          560.0794\n  Middleweight 154.0794          343.9206\n\n\n\n\n\n\n\n\nExercise 5\n\n\n\nUse the table of expected counts to answer the following questions:\n\nCompute the marginal expected counts (i.e. the total expected in rows and columns. How do these values compare to the marginal observed counts shown in the contingency table?\nCompute the row percentages for the expected table. What do you notice about these percentages? Which row percentages from the contingency table presented earlier do they match? How does confirm the expected counts are based on the null hypothesis?\nHow do the expected counts compare to the observed counts?\n\n\n\n\n\n\n\n\n\nComputing Expected Counts\n\n\n\nA simple method to compute the expected count for each cell is the following formula:\n\\(E = (r \\times c) / n\\)\nWhere r is the row total, c the column total, and n the grand total for the entire sample.\nFor fun, use this formula to compute the expected counts for one of the cells in our contingency table!\n\n\n\n\n\n\n\n\nChi-square test assumption\n\n\n\nThe chi-square test is not valid if expected counts are too small. A general rule of thumb is that they should be greater than or equal to 5.\nIn our example, the expected counts are all well above 5. We will explore this issue further in our next example.\n\n\n\n\nTest results\nThe observed counts are unlikely to exactly match the expected (this is a sample). However, if the null hypothesis is true we would anticipate the reasonably close agreement. The Chi-square test statistic, \\(X^2\\), measures how well they agree as:\n\\[X^2 = \\sum \\frac{(O-E)^2}{E}\\] In this expression, the \\(\\sum\\) symbol means “sum” or add. The terms added involve the observed (O) and expected (E) counts. In our case, there are 4 such quantities (one for each cell of the tables). For example, for the first cell (row 1, column 1) the quantity computed is:\n\\[\\frac{(241-250.9206)^2}{250.9206} = 0.39\\]\nWe calculate in similar fashion for the other three cells, and then sum (add) the four values. The resulting value, 1.3462, is given in the output from the test “X-squared”.\n\nwt_decision.chisq\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  data1$wtClass and data1$decision\nX-squared = 1.3462, df = 1, p-value = 0.2459\n\n\nIf the observed perfectly matches the expected in all cells the \\(X^2\\) value would be 0. Of course, that is unlikely. However, if the null hypothesis is true, a smaller value is more likely. But how small is reasonable? Put another way, how large a value - reflecting big differences in the observed and expected - would make us question whether the null hypothesis is true?\nWe answer this question, statistically, by calculating the probability of obtaining a value of \\(X^2\\) of 1.3462 (our observed value) or greater assuming the null hypothesis is true. If the null hypothesis is true, the distribution of possible \\(X^2\\) values follows a \\(\\chi^2\\) (“Chi-squared”) distribution. There is one parameter for this distribution, known as the “degrees of freedom” (df). For an r x c contingency table:\n\\[df = (r -1) \\times (c-1)\\]\nSince \\(r = c = 2\\) in our example, \\(df = (2-1) \\times (2-1) = 1\\).\nWith the distribution defined, we can compute the probability. The output from R gives us the result as the p-value of 0.2459. If the null hypothesis is true, we have a probability of roughly 0.25 of obtaining observed/expected differences as great as observed in our sample.\nTypically, the null hypothesis is “rejected” if the p-value is pretty small. A common cutoff is 0.05. Since our value is not that small, we cannot reject the null hypothesis.\n\n\n\n\n\n\nIMPORTANT SUMMARY: Chi-square test conclusion\n\n\n\nWe lack evidence to conclude weight class is associated with whether a decision is needed for the fight.\n\n\n\n\n\n\n\n\nExercise 6\n\n\n\nThe conclusion from the Chi-square test is that the sample does not provide enough evidence to say weight class is assocated with the decision type. How does this result compare to your intuition from looking at tables and plots earlier?",
    "crumbs": [
      "Home",
      "Mma",
      "MMA Fight Decisions"
    ]
  },
  {
    "objectID": "mma/mma_chisquare/index.html#chi-square-test",
    "href": "mma/mma_chisquare/index.html#chi-square-test",
    "title": "MMA Fight Decisions",
    "section": "Chi-square Test",
    "text": "Chi-square Test\nWe can proceed with the Chi-square test. However, notice that R responds with a warning message. The test results are still produced and in this case we again cannot conclude evidence of an association (p = 0.1851). Notice the degrees of freedom for this example. With 4 weight class categories, we have \\((4-1)\\times(2-1) = 3\\) degrees of freedom for the test.\n\nwt_decision2.chisq &lt;- chisq.test(data2$wtClass, data2$decision)\n\nWarning in chisq.test(data2$wtClass, data2$decision): Chi-squared approximation\nmay be incorrect\n\nwt_decision2.chisq \n\n\n    Pearson's Chi-squared test\n\ndata:  data2$wtClass and data2$decision\nX-squared = 4.8241, df = 3, p-value = 0.1851\n\n\nThe warning message is due to the assumption about expected cell counts. These values are well below the rule of thumb of five for both categories in the atomweight and strawweight classes.\n\nwt_decision2.chisq$expected\n\n              data2$decision\ndata2$wtClass   Decision KO/TKO/Submission\n  Atomweight   0.4848485         0.5151515\n  Bantamweight 7.2727273         7.7272727\n  Flyweight    7.7575758         8.2424242\n  Strawweight  0.4848485         0.5151515",
    "crumbs": [
      "Home",
      "Mma",
      "MMA Fight Decisions"
    ]
  },
  {
    "objectID": "mma/mma_chisquare/index.html#fisher-exact-test",
    "href": "mma/mma_chisquare/index.html#fisher-exact-test",
    "title": "MMA Fight Decisions",
    "section": "Fisher Exact Test",
    "text": "Fisher Exact Test\nWhen the assumption for the Chi-square test is not met, all is not lost! We can instead compute the p-value “exactly” using probability theory (the “hypergeometric” distribution is used, but we will omit the details).\nThe Fisher’s Exact Test assumes the row and column totals are fixed, then calculates the probability of a particular set of cell counts, then figures out all possible tables that could be constructed given the fixed totals, and calculates probabilities for each of these tables. The test determines which of these tables are extreme or more extreme than the table observed, and sums up the probabilities for the table seen and the more extreme tables to get the p-value.\nThe hypotheses for this test are the same as for the Chi-square test:\n\\(H_0\\): Weight class and the fight decision are not associated.\n\\(H_a\\): Weight class and the fight decision are associated.\nThe command is changed to “fisher.test”, but the general format of the command and output is unchanged.\n\nwt_decision2.fisher &lt;- fisher.test(data2$wtClass, data2$decision)\nwt_decision2.fisher\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  data2$wtClass and data2$decision\np-value = 0.09378\nalternative hypothesis: two.sided\n\n\nThe test returned a p-value of 0.09378. Our conclusion then after performing a Fisher’s Exact Test is again we do not have evidence to suggest that weight class and fight decision are associated (p = 0.09). In other words, we fail to reject our null hypothesis.\nNotice that the p-value is changed by a fair amount in this example even though the conclusion is unchanged. The issue with the Chi-square assumption had an impact on the calculation in this case.\n\n\n\n\n\n\nFisher Exact Test issues\n\n\n\nThe Fisher Exact test requires a great deal of computation. Every possible table of counts must be formed. For larger tables, this computation (even with the advances in computing power) might not alway be feasible.\nChi-square tests notoriously lack “power” (they may not produce statistically significant results even when there is an association). The exact test power is often even lower. In fact, in some situations with small samples there is no chance the test could lead to rejecting the null hypothesis!",
    "crumbs": [
      "Home",
      "Mma",
      "MMA Fight Decisions"
    ]
  },
  {
    "objectID": "mma/mma_chisquare/index.html#examining-the-sample",
    "href": "mma/mma_chisquare/index.html#examining-the-sample",
    "title": "MMA Fight Decisions",
    "section": "Examining the Sample",
    "text": "Examining the Sample\nBelow we select a larger sample of years and weight classes, and produce the output shown in previous examples. Use the output to answer the questions in the exercise.\n\n# Select, filter, and prepare relevant data for analysis\ndata3 &lt;- mma |&gt; \n  filter(year &gt;= 2020) |&gt;\n  filter(wtClass %in% c(\"Strawweight\", \"Flyweight\",\n                              \"Bantamweight\", \"Welterweight\",\n                              \"Middleweight\", \"Heavyweight\"))  |&gt; select(year, wtClass, decision_group) |&gt; \n  mutate(decision = if_else(decision_group == \"Decision\",\n                            \"Decision\",\n                            \"KO/TKO/Submission\"))\n\n\ntable(data3$wtClass, data3$decision)\n\n              \n               Decision KO/TKO/Submission\n  Bantamweight      302               350\n  Flyweight         248               263\n  Heavyweight       241               570\n  Middleweight      164               334\n  Strawweight       123                80\n  Welterweight      270               440\n\n\n\nlibrary(ggmosaic)\nggplot(data3) + \n  geom_mosaic(aes(x = product(wtClass, decision), fill = wtClass)) +\n  xlab('Decision')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 7\n\n\n\n\nWhat is the research question for the data selected?\nWhat are the degrees of freedom that will be used in a Chi-square test with this sample?\nDo you think the Chi-square test assumption will be a concern with this data? Explain.\nDoes the data seem to suggest an association? Explain.",
    "crumbs": [
      "Home",
      "Mma",
      "MMA Fight Decisions"
    ]
  },
  {
    "objectID": "mma/mma_chisquare/index.html#conducting-the-test",
    "href": "mma/mma_chisquare/index.html#conducting-the-test",
    "title": "MMA Fight Decisions",
    "section": "Conducting the Test",
    "text": "Conducting the Test\nAgain, we provide output from a test. Use this to answer the exercise questions.\n\ncase1.chisq &lt;- chisq.test(data3$wtClass, data3$decision)\ncase1.chisq$expected\n\n              data3$decision\ndata3$wtClass   Decision KO/TKO/Submission\n  Bantamweight 259.64431          392.3557\n  Flyweight    203.49424          307.5058\n  Heavyweight  322.96248          488.0375\n  Middleweight 198.31728          299.6827\n  Strawweight   80.84018          122.1598\n  Welterweight 282.74151          427.2585\n\ncase1.chisq\n\n\n    Pearson's Chi-squared test\n\ndata:  data3$wtClass and data3$decision\nX-squared = 109.58, df = 5, p-value &lt; 2.2e-16\n\n\n\n\n\n\n\n\nExercise 8\n\n\n\n\nWhat hypotheses for the test?\nWhat are your conclusions based on the test?\nIs the test appropriate (assumptions met) or is another test required?",
    "crumbs": [
      "Home",
      "Mma",
      "MMA Fight Decisions"
    ]
  },
  {
    "objectID": "mma/mma_chisquare/index.html#examining-the-results",
    "href": "mma/mma_chisquare/index.html#examining-the-results",
    "title": "MMA Fight Decisions",
    "section": "Examining the Results",
    "text": "Examining the Results\nThe statistically significant results from the test in this case study beg some questions.\n\n\nWhich weight classes and which decision groups differ?\n\n\nWe achieved statistical significance, but perhaps this is simply do to the very large sample size. Is there practical significance?\n\n\nWe can do an informal inspection of the observed and expected values (and also consider the visualization). We have already produced the observed (contingency) and expected tables. We add the row percentages below.\n\nPercTable(data3$wtClass, data3$decision, rfrq = \"010\",\n                     margins = c(1,2))\n\n                                                         \n                       Decision   KO/TKO/Submission   Sum\n                                                         \nBantamweight   freq         302                 350   652\n               p.row      46.3%               53.7%     .\n                                                         \nFlyweight      freq         248                 263   511\n               p.row      48.5%               51.5%     .\n                                                         \nHeavyweight    freq         241                 570   811\n               p.row      29.7%               70.3%     .\n                                                         \nMiddleweight   freq         164                 334   498\n               p.row      32.9%               67.1%     .\n                                                         \nStrawweight    freq         123                  80   203\n               p.row      60.6%               39.4%     .\n                                                         \nWelterweight   freq         270                 440   710\n               p.row      38.0%               62.0%     .\n                                                         \nSum            freq       1'348               2'037 3'385\n               p.row      39.8%               60.2%     .\n                                                         \n\n\n\n\n\n\n\n\nExercise 9\n\n\n\n\nWhat categories are different and likely led to the statistically signficant result?\nHow easy is it to identify the important differences in observed and expected values (consider this question in a table with more than two decision categories!)?\n\n\n\n\nResiduals\nWhen we have a high number of rows and columns it is difficult to see if there are practical differences between certain cells. This may be achievable with a 2x2 table, but is much more difficult with larger tables. To help with this problem, we can calculate the Pearson standardized residuals which help identify cells we should investigate.\nThe formula for the Pearson residuals is:\n\\[\\frac{(O-E)}{\\sqrt(E)}\\]\nNotice this is the square root of the “contribution” of each cell to the overall test statistic \\(X^2\\). Essentially, the formula identifies cells that have large differences between observed and expected (the \\((O-E)\\) in the formula) but “standardizes” these differences (the \\(sqrt(E)\\) in the formula).\nOne way to think about the standardization is to consider a simple example. Suppose we have two cells with \\(O-E = 2\\). However, one cell has an expected count of 10 and the other an expected count of 1,000. In the first case, the \\(O-E\\) difference is 20% of the expected value. In the second, it is only 0.2% which is actually a small amount. The standardization basically puts these on the same scale.\nStandardized residuals roughly follow a “normal distribution” which means we would expect approximately 95% of the values to between -2 and 2 and 99% between -3 and 3. Thus, values outside these ranges are somewhat unusual and cells that may contribute to a statistical association.\nWe can output the residuals from the Chi-square test in similar fashion to the expected counts:\n\ncase1.chisq$residuals\n\n              data3$decision\ndata3$wtClass    Decision KO/TKO/Submission\n  Bantamweight  2.6285868        -2.1383153\n  Flyweight     3.1198964        -2.5379882\n  Heavyweight  -4.5607794         3.7101246\n  Middleweight -2.4368714         1.9823578\n  Strawweight   4.6890531        -3.8144733\n  Welterweight -0.7577501         0.6164182\n\n\n\n\n\n\n\n\nExercise 10\n\n\n\n\nBased on the residuals what categories are different and likely led to the statistically signficant result?\nConsidering the categories you identified, do the differences seem practically meaningful? Explain.",
    "crumbs": [
      "Home",
      "Mma",
      "MMA Fight Decisions"
    ]
  },
  {
    "objectID": "anw/kaplan_meier/index.html",
    "href": "anw/kaplan_meier/index.html",
    "title": "American Ninja Warrior - Kaplan-Meier Survival Analyis",
    "section": "",
    "text": "Facilitation notes\n\n\n\n\n\n\nThis module would be suitable for an in-class lab or take-home assignment in an intermediate statistics course.\nIt assumes a familiarity with the RStudio Environment and R programming language.\nStudents should be provided with the following data file (.csv) and Quarto document (.qmd) to produce visualizations and write up their answers to each exercise. Their final deliverable is to turn in an .html document produced by “Rendering” the .qmd.\n\ndata\nStudent Quarto template\n\nPosit Cloud (via an Instructor account) or Github classroom are good options for disseminating files to students, but simply uploading files to your university’s course management system works, too.",
    "crumbs": [
      "Home",
      "Anw",
      "American Ninja Warrior - Kaplan-Meier Survival Analyis"
    ]
  },
  {
    "objectID": "anw/kaplan_meier/index.html#terms-to-know",
    "href": "anw/kaplan_meier/index.html#terms-to-know",
    "title": "American Ninja Warrior - Kaplan-Meier Survival Analyis",
    "section": "Terms to know",
    "text": "Terms to know\nBefore proceeding with the survival analysis, let’s make sure we understand American Ninja Warrior and some of it’s vocabulary to help us climb our way through this lab.\n\nHow does American Ninja Warrior work?\nAmerican Ninja Warrior is an NBC competition show where participants attempt to complete a series of obstacle courses of increasing difficulty. In a single obstacle course, the competitors must complete a series of obstacles in a row. If they fail an obstacle (usually this happens when they fall into the water below), they are eliminated from the competition. The competitors also have a time limit to complete the course. The competitors are ranked based on how far they get in the course and how quickly they complete it.\nMost of the obstacles are designed to test the competitors’ upper body strength. Some obstacles require balance and agility though.\n\n\nThe warped wall is arguably the most famous, although now least difficult, obstacle on an American Ninja Warrior course. The warped wall is a curved wall that competitors must run up and grab the top of. The warped wall is on every course and is often the final obstacle, although this is not the case on the Finals courses.\nThe warped wall was previously 14 feet and is now 14.5 feet tall. They have even had a 18 foot warped wall on the show.\nWarped Wall \nImage Source: SB Nation\nThe obstacles in American Ninja Warrior are all given names. For example, the famed Warped Wall is a curved wall that competitors must run up and grab the top of. The Salmon Ladder is a series of rungs that competitors must move up by jumping and pulling themselves up.\nWatch Enzo Wilson complete the American Ninja Warrior course at the 2021 Finals Stage 1 (Season 13) in the video below.\n\n\n\n\n\n\n\nKey Terms\n\n\n\n\nObstacle: A challenge that competitors must complete to move on in the competition.\nCourse: A series of obstacles that competitors must complete in a row. A typical course has 6-10 obstacles.\nStage: A round of the competition. The competition starts with Stage 1 and progresses to Stage 4.\nTime Limit: The amount of time competitors have to complete the course, often between 2-4 minutes.",
    "crumbs": [
      "Home",
      "Anw",
      "American Ninja Warrior - Kaplan-Meier Survival Analyis"
    ]
  },
  {
    "objectID": "anw/kaplan_meier/index.html#variable-descriptions",
    "href": "anw/kaplan_meier/index.html#variable-descriptions",
    "title": "American Ninja Warrior - Kaplan-Meier Survival Analyis",
    "section": "Variable Descriptions",
    "text": "Variable Descriptions\nThe ninja data you’ll be analyzing in this lab provides the individual run information for each ninja in the 2021 Finals Stage 1 (Season 13). The data includes the ninja’s name, their sex, the obstacle they failed on, and the cause of that failure.\n\n\nVariable Descriptions\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nname\nName of the American Ninja Warrior\n\n\nsex\nSex of the American Ninja Warrior (M/F)\n\n\nobstacle\nThe name of the obstacle the ninja failed on\n\n\nobstacle_number\nThe obstacles’ place in the run\n\n\ncause\nWhat caused the ninja to fail (Fall/Time/Complete)",
    "crumbs": [
      "Home",
      "Anw",
      "American Ninja Warrior - Kaplan-Meier Survival Analyis"
    ]
  },
  {
    "objectID": "anw/kaplan_meier/index.html#points-of-confusion",
    "href": "anw/kaplan_meier/index.html#points-of-confusion",
    "title": "American Ninja Warrior - Kaplan-Meier Survival Analyis",
    "section": "Points of Confusion",
    "text": "Points of Confusion\nUse the following code to create a table of the obstacle names and their corresponding obstacle numbers. This will help you understand the order of the obstacles in the course.\n\nninja |&gt; \n  distinct(obstacle, obstacle_number)\n\n# A tibble: 10 × 2\n   obstacle        obstacle_number\n   &lt;chr&gt;                     &lt;dbl&gt;\n 1 Slide Surfer                  1\n 2 Swinging Blades               2\n 3 Double Dipper                 3\n 4 Jumping Spider                4\n 5 Tire Run                      5\n 6 Dipping Birds                 7\n 7 The High Road                 8\n 8 Fly Hooks                     8\n 9 Cargo Net                     9\n10 Complete                     10\n\n\nIt can be seen quickly that there is no obstacle number 6 in the data and that there are 2 obstacles with number 8. Both of these appear confusing at first, but they have logical explanations.\nThe missing obstacle number 6 is due to the fact that no ninja failed on obstacle 6. Some quick research tells us that obstacle 6 was the warped wall.\n\n\nRead about the 2021 Stage 1 Split Decision here.\nThe duplicate obstacle number 8 is due to the fact that Stage One of the 2021 Finals allowed a split-decision. This means that competitors could choose between two different obstacles for obstacle 8.\nAnother important thing to note is that obstacle 10 is not really an obstacle, but rather the end of the course.\nLastly, one competitor Joe Moravsky ran the course twice (falling the first time and completing it the second time). This is because he was the Safety Pass Winner from the previous round. The Safety Pass allows a competitor to run the course again if they fail the first time. This poses some questions about how to handle this observation. We could\n\nInclude both runs in the analysis, treating them as separate observations.\nInclude only the first run in the analysis.\nInclude only the second run in the analysis.\n\nIf we include the second run in the analysis, we are neglecting the fact that Joe Moravsky had already attempted the course once and may have learned from his mistakes.\nIf we include the first run in the analysis, an argument could be made that Moravsky only failed the first time because he knew he had a second chance.\nIn most survival analysis situations, an individual would not be capable of participating twice from the beginning (after all if death were truly the event of interest, it would be safe to say there is no second chance). Therefore, we will only include the first run in the analysis.\nRun the code below to remove the second run from the data.\n\nninja &lt;- ninja |&gt; \n  filter(name != \"Joe Moravsky (Safety Pass)\")\n\nNow that we’ve cleared some of the muddiness, let’s move on to the fun stuff!",
    "crumbs": [
      "Home",
      "Anw",
      "American Ninja Warrior - Kaplan-Meier Survival Analyis"
    ]
  },
  {
    "objectID": "anw/kaplan_meier/index.html#censored-data",
    "href": "anw/kaplan_meier/index.html#censored-data",
    "title": "American Ninja Warrior - Kaplan-Meier Survival Analyis",
    "section": "Censored Data",
    "text": "Censored Data\nIn survival analysis, we often have need to censor data. Censored data occurs when the event of interest has not occurred for some of the observations. In our case, the event of interest is the failure of a competitor on an obstacle. If a competitor completes the course, we do not know which obstacle they would have failed on. Additionally if the time limit is reached, we do not know which obstacle the competitor would have failed on, although we do know that they didn’t fail on any of the ones they completed before time was up. Both of these situations are examples of censored data.\nUse the code below to create a new column called censor in the ninja data that is a binary indicator of whether or not the observation should be censored. This column will be used to indicate whether the data is censored or not.\n\n# Makes a column called censor that is 1 if the competitor failed and 0 if they completed the course or ran out of time\nninja &lt;- ninja |&gt; \n  mutate(censor = if_else(cause %in%  c(\"Complete\", \"Time\"), 0, 1))",
    "crumbs": [
      "Home",
      "Anw",
      "American Ninja Warrior - Kaplan-Meier Survival Analyis"
    ]
  },
  {
    "objectID": "anw/kaplan_meier/index.html#calculating-survival-probabilities",
    "href": "anw/kaplan_meier/index.html#calculating-survival-probabilities",
    "title": "American Ninja Warrior - Kaplan-Meier Survival Analyis",
    "section": "Calculating Survival Probabilities",
    "text": "Calculating Survival Probabilities\nThe Kaplan-Meier estimator uses information from all of the observations in the data and considers survival to a certain point in time as a series of steps defined at the observed survival times.\nIn order to calculate the probability of surviving past a certain point in time (past a certain obstacle in this case), the conditional probability of surviving past that point given that the competitor has survived up to that point must be calculated first.\nThe formula for the conditional probability of surviving past a point in time (\\(t_i\\)) given that the competitor has survived up to that point in time(\\(t_{i-1}\\)) is:\n\n\nNote: This function could also be written as \\(P(T &gt; t_i | T \\geq t_{i-1}) = \\frac{n_i - d_i}{n_i}\\)\n\\(P(T \\geq t_i | T \\geq t_{i-1}) = 1- \\frac{d_i}{n_i}\\)\nWhere:\n\n\\(d_i\\) is the number of competitors that failed at time \\(t_i\\)\n\\(n_i\\) is the number of competitors that were at risk at time \\(t_i\\)\n\nThe Kaplan-Meier estimator is the product of the conditional probabilities of surviving past each point in time up through that point in time.\n\\(\\hat{S}(t) = \\prod_{t_i \\leq t} (1 - \\frac{d_i}{n_i})\\)\n\n\nNote: Censored data does not count in the at risk competitors\nwhere \\(n_i = n_{i-1} - d_{i-1} - c_{i-1}\\)\nFor example if we have this data:\n\n# Setting a seed for reproducibility\nset.seed(123)\n\n# Creating fake data\nfake_data &lt;- tibble(obstacle_number = c(1:5, 4:5), censor = c(rep(1, 5), rep(0, 2))) |&gt; \n  sample_n(25, replace = TRUE)\n\nhead(fake_data)\n\n# A tibble: 6 × 2\n  obstacle_number censor\n            &lt;int&gt;  &lt;dbl&gt;\n1               5      0\n2               5      0\n3               3      1\n4               4      0\n5               3      1\n6               2      1\n\n\nAnd we wanted to calculate the Kaplan-Meier estimate of surviving past obstacle 2 we would need to find the following probabilities:\n\\(P(T &gt; 1 | T &gt; 0) = P(T &gt; 1) =  1 - \\frac{\\text{number of competitors that failed at obstacle 1}}{\\text{number of competitors that attempted obstacle 1}}\\)\n\\(P(T &gt; 2 | T &gt; 1) = 1 - \\frac{\\text{number of competitors that failed at obstacle 2}}{\\text{number of competitors that attempted obstacle 2}}\\)\nThe following code calculates the first two probabilities:\n\nfake_data_summary &lt;- fake_data |&gt; \n  filter(obstacle_number &lt;= 2) |&gt; \n  group_by(obstacle_number) |&gt; \n  summarize(fails = sum(censor == 1)) |&gt; \n  ungroup() |&gt;\n  mutate(at_risk = 25 - cumsum(fails),\n         cond_prob = 1 - fails/at_risk)\n\nfake_data_summary\n\n# A tibble: 2 × 4\n  obstacle_number fails at_risk cond_prob\n            &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1               1     4      21     0.810\n2               2     3      18     0.833\n\n\nWe would then need to multiply these two probabilities together to get the Kaplan-Meier estimate of surviving past obstacle 2.\n\\(P(T &gt; 2) = P(T &gt; 1) * P(T &gt; 2 | T &gt; 1)\\)\nThe following code calculates the Kaplan-Meier estimate of surviving past obstacle 2:\n\nkaplan_meier &lt;- fake_data_summary |&gt; \n  summarize(kaplan_meier = prod(cond_prob))\n\nThe Kaplan-Meier estimate of surviving past obstacle 2 in this fake example is 0.6746032.",
    "crumbs": [
      "Home",
      "Anw",
      "American Ninja Warrior - Kaplan-Meier Survival Analyis"
    ]
  },
  {
    "objectID": "anw/kaplan_meier/index.html#kaplan-meier-estimator-manual-calculation",
    "href": "anw/kaplan_meier/index.html#kaplan-meier-estimator-manual-calculation",
    "title": "American Ninja Warrior - Kaplan-Meier Survival Analyis",
    "section": "Kaplan-Meier Estimator Manual Calculation",
    "text": "Kaplan-Meier Estimator Manual Calculation\nThe ninja data frame contains information about individual competitors in the ninja competition. We will need to summarize the data to calculate the Kaplan-Meier estimator manually.\n\n\n\n\n\n\nExercise 2: Manual Calculation of Kaplan-Meier Estimator\n\n\n\nIn this exercise you will calculate the Kaplan-Meier estimator of surviving past each obstacle in the ninja competition step-by-step.\n\n\n\n\n\n\nPart 1: Number of Events\n\n\n\nThe first step is to calculate the number of competitors that failed and the number of competitors that were censored at each point in time. These are the \\(d_i\\) and \\(c_i\\) values needed to calculate the conditional probability of surviving past each point in time.\nUse the following code to sum the number of competitors that failed and the number of competitors that were censored at each obstacle.\n\n\nninja_summary &lt;- ninja |&gt; \n  group_by(obstacle = obstacle_number) |&gt;\n  summarize(fails = sum(cause == \"Fall\"),\n            censored = sum(cause %in% c(\"Complete\", \"Time\")))\n\n\nAt which obstacle did the most competitors fail?\nAt which obstacle were the most competitors censored (not including obstacle 10 which is completion)?\n\n\n\n\n\n\n\n\n\nPart 2: At Risk Competitors\n\n\n\nThe second step is to calculate the number of competitors at risk at each point in time. This is the \\(n_i\\) value needed to calculate the conditional probability of surviving past each point in time.\nUse the following code to calculate the number of competitors at risk at each point in time from the ninja_summary data frame.\n\n\nninja_summary &lt;- ninja_summary |&gt; \n  mutate(attempts = 69 - lag(cumsum(fails), default = 0) - cumsum(censored))\n\n\nWhich obstacle had the most competitors at risk?\nWhy don’t censored competitors contribute to the number of competitors at risk at the obstacle?\n\n\n\n\n\n\n\n\n\nPart 3: Conditional Survival Probability\n\n\n\nThe third step is to calculate the conditional probability of survival at each point in time. This is the \\(P(T \\geq t_i | T \\geq t_{i-1})\\) value needed to calculate the Kaplan-Meier estimator and is calculated as \\(1 - \\frac{d_i}{n_i}\\).\n\nUse the ninja_summary data frame to calculate the probability that someone survives each obstacle. Do this using the mutate function to create a new column called surv_prob. Survival probability is 1 minus the number of competitors that failed divided by the number of competitors at risk. Save this data frame as ninja_summary.\n\n\n\nWhat percentage of at-risk competitors survived the first obstacle?\nWhat percentage of at-risk competitors failed the fifth obstacle?\nWhich obstacle had the highest conditional fail probability?\nDid obstacle 2 or obstacle 7 have a higher conditional survival rate?\n\n\n\n\n\n\n\n\n\nPart 4: Kaplan-Meier\n\n\n\nThe final step is to calculate the Kaplan-Meier estimator of surviving past each point in time (\\(\\hat{S}(t)\\)) This is calculated as the product of the conditional probabilities of surviving past each point in time up through the desired point in time (\\(\\prod_{i=1}^{t} P(T \\geq t_i | T \\geq t_{i-1})\\)).\nUse the following code to calculate the Kaplan-Meier estimator manually by multiplying the conditional probabilities of surviving past each point in time up through the desired point in time.\n\n\nninja_summary &lt;- ninja_summary |&gt; \n  mutate(km = cumprod(surv_prob))\n\n\nWhat is the Kaplan-Meier estimate for surviving past the first obstacle?\nWhat is the Kaplan-Meier estimate for surviving past first five obstacles?\nWhat is the farthest obstacle that for which the Kaplan-Meier estimator has more 50% of competitors surviving?\n\n\n\n\nPlotting the Kaplan-Meier Estimator\nWe will now use ggplot2 to plot the Kaplan-Meier estimator for the ninja competitors. The Kaplan-Meier estimator is a step function, so we will use geom_step to plot the estimator. We will also use geom_point to plot the points where the estimator changes.\n\n\n\n\n\n\n\nPart 5: Plotting the Kaplan-Meier Estimator\n\n\n\n\n\nUse the ninja_summary data frame in conjunction with ggplot2’s geom_step and geom_point to plot the Kaplan-Meier estimator for the ninja competitors.\nComment on the plot.\nWhat do you notice about where the lowest point on the plot is in regard to survival probability? Does survival probability reach zero? Why or why not?\n\n\n\n\n\n\n Note: The censored column is the number of competitors that were censored at each obstacle. The only reason that this number is not 0 at any obstacle that is not number 10 (completed) is because some competitors ran out of time on that obstacle.\n\n\nNote: The lag function shifts the cumsum of the fails column down one row. The default = 0 argument fills in the first row with 0. This is necessary to help calculate the number of competitors at risk at each obstacle. Note that the lag function is not used in conjunction with the cumsumfunction for the censored column.\n\n TIP: You can pipe the data frame into the mutate function to create a new column.\nTIP: The mutate function works like so: data_frame |&gt; mutate(new_column = calculation)\n\n Note: The cumprod function calculates the cumulative product of the values given to it.\n\n Type ?geom_step, ?geom_point, or ?ggplot in the console to learn more about these functions.\n\nTIP: Remember that you can add the + operator to continue adding layers to the plot like seen below\n\nggplot(your_data, aes(x = time_var, y = kaplan_meier_var)) +\n  geom_step() +\n  geom_point()\n\nTIP: You can also add labels to the plot using the labs function like seen below\n\nggplot(your_data, aes(x = time_var, y = kaplan_meier_var)) +\n  geom_step() +\n  geom_point() +\n  labs(title = \"Your Title\",\n       x = \"X Axis Label\",\n       y = \"Y Axis Label\")",
    "crumbs": [
      "Home",
      "Anw",
      "American Ninja Warrior - Kaplan-Meier Survival Analyis"
    ]
  },
  {
    "objectID": "anw/kaplan_meier/index.html#using-r-to-calculate-the-kaplan-meier-estimator",
    "href": "anw/kaplan_meier/index.html#using-r-to-calculate-the-kaplan-meier-estimator",
    "title": "American Ninja Warrior - Kaplan-Meier Survival Analyis",
    "section": "Using R to Calculate the Kaplan-Meier Estimator",
    "text": "Using R to Calculate the Kaplan-Meier Estimator\nPhew! That was a lot of tedious work to calculate and plot the Kaplan-Meier estimator manually. Luckily, there is a much easier way to calculate the Kaplan-Meier estimator using R.\nThe survival package in R provides a function called survfit that can be used to calculate the Kaplan-Meier estimator. The survfit function requires a Surv object as input. The Surv object is created using the Surv function, which requires two arguments:\n\nThe time to event data. The time to event data is the time at which the event occurred or the time at which the individual was censored. In our case this is the obstacle_number in our ninja data.\nThe event status. The event status is a binary variable that indicates whether the event occurred or the individual was censored. The event status is coded as 1 if the event occurred and 0 if the individual was censored. This is contained in the censor column of the ninja data.\n\nBelow a survfit model is created for the ninja dataset and the results are stored in the ninja_km object.\n\nninja_km &lt;- survfit(Surv(obstacle_number, censor) ~ 1, data = ninja)\n\n\n\n\n\n\n\nExercise 3: Kaplan-Meier Estimates and Interpretation\n\n\n\nUse summary(ninja_km) to view a summary of the Kaplan-Meier estimator.\n\nDo the values in the survival column match the values you calculated manually?\n\n\nThe output also shows the 95% confidence intervals.\n\nWhich obstacle number is the first point in time where a survival rate of less than .5 falls within the 95% confidence interval?\nWhat do you notice about the standard error as the time increases?\n\n\n\n\nThe computations for calculating the Confidence Interval for the K-M Estimate are fairly complex. The method most commonly used is called the log-log survival function and was proposed by Kalbfleisch and Prentice (2002). This function is computed by \\(ln(-ln[\\hat{S}(t)])\\) with variance derived from the delta method and calculated by \\[\n\\frac{1}{[ln(\\hat{S}(t))]^2}\\sum_{t_i\\leq{t}}\\frac{d_i}{n_i(n_i - d_i)}\n\\].\nThe endpoints for the confidence interval for the log-log survival function are therefore found by \\(ln(-ln[\\hat{S}(t)]) \\pm Z_{1-\\alpha / 2} SE [ln(-ln[\\hat{S}(t)]) ]\\)\nAnd the endpoints expressed by the computer and seen in the summary are \\(exp[-exp(\\hat{c}_u)] \\text{ and } exp[-exp(\\hat{c}_l)]\\)\n\nQuartile Interpretation\nThe three quartiles are common statistics to look at when doing a survival analysis. The interpretations of these are as follows:\n\n\nNote: If the data is uncensored the estimate is just the median of the data. If the data is censored, the KM estimate is used to find these by finding the time at which it drops below the percentile\n\n25th Percentile- 75% of the people survive past this point in time\nMedian- 50% of the people will survive past this time\n75th Percentile- 25% survive past this time\n\n\n\n\n\n\n\nExercise 4: Interpreting Quartiles\n\n\n\nUse the results from quantile(ninja_km) to answer the following questions\n\nWhat is the earliest time that the confidence intervals imply that the true mean of surviving past that time could be 75%? What is the latest time?\nWhat is the interpretation of the NA values in the 75th percentile columns?\nWhat is the earliest time (within the 95% confidence interval) at which the true survival rate suggests 50% of the competitors would fail on or before?\n\n\n\n\n\nPlotting with R\nAfter fitting a Kaplan-Meier model, we can use the ggsurvplot function from the survminer package to plot the Kaplan-Meier estimator. The ggsurvplot function requires the Kaplan-Meier model as input.\nBelow is an example of how easy it is to plot the Kaplan-Meier estimator using R.\n\nggsurvplot(ninja_km,\n           conf.int = TRUE)",
    "crumbs": [
      "Home",
      "Anw",
      "American Ninja Warrior - Kaplan-Meier Survival Analyis"
    ]
  },
  {
    "objectID": "anw/kaplan_meier/index.html#the-log-rank-test",
    "href": "anw/kaplan_meier/index.html#the-log-rank-test",
    "title": "American Ninja Warrior - Kaplan-Meier Survival Analyis",
    "section": "The Log-Rank Test",
    "text": "The Log-Rank Test\nThe Log-Rank Test is a statistical test used to compare the survival probabilities of two or more groups. The test is used to determine if there is a statistically significant difference between the survival probabilities of the groups.\nThe hypotheses for our log-rank test are as follows:\n\n\\(H_0: S_M(t) = S_F(t)\\) for all \\(t\\)\n\\(H_a: S_M(t) \\neq S_F(t)\\) for at least one \\(t\\)\n\nwhere \\(S_M(t)\\) is the survival probability for males at time \\(t\\) and \\(S_F(t)\\) is the survival probability for females at time \\(t\\).\nWhen comparing two groups like this, we can calculate the expected number of deaths in each group. Below is the formula for calculating the number of expected deaths for group 0 at time \\(t_i\\):\n\\[\\hat{e}_{0i} = \\frac{n_{0i}d_i}{n_i}\\]\nwhere \\(n_{0i}\\) is the number of individuals at risk in group 0 at time \\(t_i\\), \\(d_i\\) is the total number of deaths at time \\(t_i\\), and \\(n_i\\) is the total number of individuals at risk at time \\(t_i\\).\nThe variance estimator is drawn from the hypergeometric distribution. The formula for the variance of the number of deaths in group 0 at time \\(t_i\\) is:\n\\[\\hat{v}_{0i} = \\frac{n_{0i}n_{1i}d_i(n_i - d_i)}{n_i^2(n_i - 1)}\\]\nwhere \\(n_{0i}\\) is the number of individuals at risk in group 0 at time \\(t_i\\), \\(n_{1i}\\) is the number of individuals at risk in group 1 at time \\(t_i\\), \\(d_i\\) is the total number of deaths at time \\(t_i\\), and \\(n_i\\) is the total number of individuals at risk at time \\(t_i\\).\nThe test statistic is calculated as the square of the sum of the differences between the observed and expected number of deaths for the group divided by the sum of the variance of the number of deaths for the group at each time point. The formula for the test statistic is as follows:\n\\[Q = \\frac{[\\sum_{i=1}^m (d_{0i} - \\hat{e}_{0i})]^2}{\\sum_{i=1}^m \\hat{v}_{0i}}\\]\nUsing the null hypothesis, the p-value can be calculated using the chi-squared distribution with 1 degree of freedom.\n\\[p = P(X^2(1) &gt; Q)\\]\n\n\nNOTE: This use of the chi-squared distribution assumes that the censoring is independent of the group.\n\nNOTE: The degrees of freedom for the chi-squared distribution is 1 because we are comparing two groups. If we were comparing more than two groups, the degrees of freedom would be the number of groups minus 1.\n\nThankfully R has a built-in function to perform the log-rank test. The survdiff function in the survival package can be used to perform the log-rank test. The survdiff function requires a Surv object as input. It will then perform the log-rank test and return the test statistic and p-value.\n\n\nNOTE: The log-rank test is a non-parametric test. This means that it does not assume that the data is normally distributed.\nThe code below runs the log-rank test on the ninja data set to compare the survival of male and female competitors.\n\nninja_km_diff &lt;- survdiff(Surv(obstacle_number,\n                               censor) ~ sex, data = ninja)\n\n\n\n\n\n\n\nExercise 6: Comparing Survival vs. Expected\n\n\n\nUse the code below to see the results of the survdiff function\n\nninja_km_diff\n\nCall:\nsurvdiff(formula = Surv(obstacle_number, censor) ~ sex, data = ninja)\n\n       N Observed Expected (O-E)^2/E (O-E)^2/V\nsex=F 12       10     4.19      8.07      10.3\nsex=M 56       27    32.81      1.03      10.3\n\n Chisq= 10.3  on 1 degrees of freedom, p= 0.001 \n\n\n\nHow many female competitors are in the data set? How many fell? How many were expected to fall (round to the nearest whole number)?\nDid more or less male competitors fall than expected?\nWhat is the p-value of the test? What does this mean?",
    "crumbs": [
      "Home",
      "Anw",
      "American Ninja Warrior - Kaplan-Meier Survival Analyis"
    ]
  },
  {
    "objectID": "anw/kaplan_meier/index.html#other-nonparametric-tests",
    "href": "anw/kaplan_meier/index.html#other-nonparametric-tests",
    "title": "American Ninja Warrior - Kaplan-Meier Survival Analyis",
    "section": "Other Nonparametric Tests",
    "text": "Other Nonparametric Tests\nAlthough the survdiff function uses the most common test for comparing Kaplan-Meier curves, there are a variety of other methods that can be used. These other methods developed because of the log rank test’s greatest weakness: It weights all time points equally even though there are fewer people at risk later than at the beginning. These methods are all similar to a standard log-rank test but attempt to weight time points in order to detect differences better throughout time as opposed to the end, which is where the log-rank test finds most of its differences. The ratio of the observed and expected number of deaths is calculated in a similar manner but with weights applied as seen below:\n\\[Q = \\frac{[\\sum_{i=1}^m w_i(d_0i - \\hat{e}_{0i})]^2}{\\sum_{i=1}^m w_i^2\\hat{v}_{0i}}\\]\nBelow some of the other methods that can be used are broken down, with their weighting and purpose explained:\n\nWilcoxon (Gehan-Breslow) Test: This test gives more weight to early time points based on the number of individuals at risk. Its weighting is: \\[w_i = n_i\\]\nTarone-Ware Test: This test gives more weight to time points with more individuals at risk, but less heavily than the Gehan-Breslow test. Its weighting is: \\[w_i = \\sqrt{n_i}\\]\nPeto-Prentice Test: This test also gives more weight to earlier time points, but not as much as the Gehan-Breslow test. Its weighting is:\n\n\\[w_i = \\tilde{S}(t_{(i)})\\] where \\[\\tilde{S}(t_{(i)}) = \\prod_{t_{(j)}&lt;t} \\left(1 - \\frac{d_j}{n_j}\\right)\\]\n\nFleming-Harrington Test: This test allows the user to chose \\(\\rho\\) and \\(q\\) values to weight the time points. If \\(\\rho\\) is larger it will weight the earlier time points more heavily, and if \\(q\\) is larger it will weight the later time points more heavily. Its weighting is:\n\n\\[w_i = [\\tilde{S}(t_{(i-1)})]^{\\rho}[1 - \\tilde{S}(t_{(i-1)})]^q\\] where \\[\\tilde{S}(t_{(i- 1)}) = \\text{Kaplan-Meier Estimate at time } t_{i-1}\\]\nThankfully the surv_pvalue function in the survminer package can be used to calculate the p-value for all of these tests by changing the method argument. See the table below for the different method arguments to use:\n\n\n\nTest\nMethod Argument\n\n\n\n\nLog Rank Test\nDefault- no argument needed\n\n\nWilcoxon/Gehan-Breslow\nmethod = “n”\n\n\nTarone-Ware\nmethod = “TW”\n\n\nPeto-Prentice\nmethod = “PP”\n\n\nFleming-Harrington\nmethod = “FH”\n\n\n\nThe surv_pvalue function does need a survfit object as input. We can use the ninja_km_gender object created earlier to check the p-values for the different methods.\n\n\n\n\n\n\nExercise 7: Log-Rank Tests\n\n\n\nRun the code below to see the p-values for the different methods.\n\nsurv_pvalue(ninja_km_gender) #log rank\nsurv_pvalue(ninja_km_gender, method = \"n\") #Gehan Breslow (generalized Wilcoxon)\nsurv_pvalue(ninja_km_gender, method = \"TW\") #tarone-ware\nsurv_pvalue(ninja_km_gender, method = \"PP\") #Peto-Prentice\nsurv_pvalue(ninja_km_gender, method = \"FH\") #Fleming-Harrington\n\n\nUsing \\(\\alpha = 0.05\\), do all of the tests lead to the same conclusion? If so what is the conclusion? If not which ones agree and which ones do not?\nWhich test had the smallest p-value?\nWhich test had the largest p-value?\nBased off of the p-values for the different tests, would you conclude that the difference between the genders is most likely more significant at the beginning or end of the course?",
    "crumbs": [
      "Home",
      "Anw",
      "American Ninja Warrior - Kaplan-Meier Survival Analyis"
    ]
  },
  {
    "objectID": "golf/pga_masters_nor/index.html",
    "href": "golf/pga_masters_nor/index.html",
    "title": "PGA - Scoring Average Confidence Intervals (No R)",
    "section": "",
    "text": "Facilitation notes\n\n\n\n\n\n\nThis module would be suitable for an in-class lab or take-home assignment in an introductory statistics course.\nThe data used to create this module and a student answer template can be downloaded from the following links:\n\ndata\nStudent Answer template",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scoring Average Confidence Intervals (No R)"
    ]
  },
  {
    "objectID": "golf/pga_masters_nor/index.html#terms-to-know",
    "href": "golf/pga_masters_nor/index.html#terms-to-know",
    "title": "PGA - Scoring Average Confidence Intervals (No R)",
    "section": "Terms to know",
    "text": "Terms to know\nBefore proceeding with the analysis, let’s make sure we know some important golf terminology that will help us master this lab.\n\nGolf Terminology\n\nPar in golf is the amount of strokes that a good golfer is expected to take to get the ball in the hole.\n\nEach hole in golf has its own par. There are par 3 holes, par 4 holes, and par 5 holes.\nThere are 18 holes on a golf course and the pars of each of these holes sums to par for the course, also known the course par.\n\nA round in golf is when a golfer plays the full set of 18 holes on the course.\n\nIn most professional golf tournaments, all golfers play 2 rounds, the best golfers are selected and those golfers play 2 more rounds for a total of 4 rounds.\n\n\n\n\n\n\n\n\nTypes of Golf Tours\n\n\n\n\nIn golf there are a few tours, better thought of as leagues, that golfers regularly compete in\n\nThe PGA, or “Professional Golf Association”, has long been considered the preeminant golfing tour, hosting most tournaments and containing the most skilled members.\nThe LIV tour is a Saudi-backed alternative to the PGA that was played its first season in 2022. LIV is the Roman numeral for 54 and is related to the fact that LIV tournaments only allow 54 players and only play 54 holes, compared to the normal PGA 72 holes.\nThe PGA Tour Champions is a branch off of the PGA tour for players 50 or older. It used to be called the “Senior PGA Tour” until 2003, when it began being called the Champions tour\nAn amateur is a golfer who is not yet a professional. They are not allowed to win money in professional golf tournaments. Most amateurs are college golfers.\n\n\n\n\n\n\nPGA vs. LIV\n\nvs\n\nImages Source: PGA and LIV\nClick here to read about LIV golf’s founding and its continued impact on the PGA tour.",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scoring Average Confidence Intervals (No R)"
    ]
  },
  {
    "objectID": "golf/pga_masters_nor/index.html#t-interval-for-single-means",
    "href": "golf/pga_masters_nor/index.html#t-interval-for-single-means",
    "title": "PGA - Scoring Average Confidence Intervals (No R)",
    "section": "t-interval for single means",
    "text": "t-interval for single means\nA t-distribution is used for calculating a single mean confidence interval if the sample size is small (rule of thumb: less than 30) and the population standard deviation is unknown.\nThe formula for calculating a CI using this method is shown below: \\[CI = \\bar{X} \\pm t_{\\alpha/2, df} \\times \\frac{S}{\\sqrt{n}}\\]\n\n\nClick here to learn more about the t-distribution and play around with t-distribution graphs.\nWhere \\(\\bar{X}\\) is the sample mean,\n\\(t_{\\alpha/2, df}\\) is the critical value for the t-distribution with \\(df = n-1\\),\n\\(S\\) is the sample standard deviation,\nand \\(n\\) is the sample size.\n\n\nNOTE: The t-distribution changes based on the degrees of freedom, approaching the normal distribution as the degrees of freedom increase.\nThe critical value for the t-distribution is determined by the confidence level and the degrees of freedom. This is calculated by finding the value of \\(t_{\\alpha/2, df}\\) such that the area under the t-distribution curve (with that specific degrees of freedom) between \\(-t_{\\alpha/2, df}\\) and \\(t_{\\alpha/2, df}\\) is equal to the confidence level.\n\n\n\n\n\n\n\n\n\nThese critical values can be found using t-tables like the one below:\n\nFor a 95% confidence interval, our Type I error rate is \\(\\alpha = 0.05\\). Since confidence intervals are two-tailed, we split this \\(\\alpha\\) in half, half in the left tail and half in the right tail. This means that we are looking for \\(t_{.025, df}\\) or \\(t_{.975, df}\\).\n\n\nNOTE: A Type I error is when we reject the null hypothesis when it is actually true. This is also known as a false positive.\n\n\n\n\n\n\nExercise 1: t-distribution confidence intervals\n\n\n\nThe table below shows summary data for the first round of the 2023 Masters tournament for amateur golfers. The critical value is for a 90% confidence interval with 6 degrees of freedom.\n\n\n\n\n\n\n\n\n\n\n\n\nSample\nSample Mean\nSample Standard Deviation\nSample Size\nCritical Value\n\n\n\n\nAmateurs Round 1\n75\n3.21455\n7\n1.94318\n\n\n\n\n\nUse the formula for creating a confidence interval using the t-distribution to calculate the upper and lower limits of a confidence interval for the true mean of amateur scoring using the amateurs in the first round as our sample. Use a 90% confidence interval (\\(\\alpha = .1\\)).\n\nWhat is the lower bound of the confidence interval?\nWhat is the upper bound of the confidence interval?\nWhat is the interpretation of this confidence interval?\n\n\n\nRepeat this process with a 99% confidence interval (\\(\\alpha = .01\\)).\n\nWhat is the new lower bound of the confidence interval?\nWhat is the new upper bound of the confidence interval?\nIs this 99% confidence interval larger, smaller, or the same as 90% confidence interval?\nWhen thinking about your anwer to f, what do you think could explain this?\n\n\n\nTIP: When interpreting a confidence interval do not say “there is a 90% chance that the true mean is between the lower and upper bounds”. Instead, say “we are 90% confident that the true mean is between the lower and upper bounds”.\n\nTIP: The t-table from earlier in the lesson will need to be used to find the critical value for the t-distribution for a 99% confidence interval with 6 degrees of freedom.",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scoring Average Confidence Intervals (No R)"
    ]
  },
  {
    "objectID": "golf/pga_masters_nor/index.html#z-interval-for-single-means",
    "href": "golf/pga_masters_nor/index.html#z-interval-for-single-means",
    "title": "PGA - Scoring Average Confidence Intervals (No R)",
    "section": "z-interval for single means",
    "text": "z-interval for single means\nA standard normal distribution (also known as a z-distribution) is used to calculate the confidence interval for a single mean if the sample size is large enough (greater than 30) or the population standard deviation is known. The first case is common as oftentimes samples are greater than 30. The second case is rare because it is uncommon to know the population standard deviation but not the population mean.\nThe formula for the confidence interval for a single mean using the z-distribution is very similar to that of the t-distribution\n\n\nClick here for more information about the standard normal distribution.\n\\(CI = \\bar{X} \\pm Z_{\\alpha/2} \\times \\frac{\\sigma}{\\sqrt{n}}\\)\nWhere \\(\\bar{X}\\) is once again the sample mean, \\(Z_{\\alpha/2}\\) is the critical value for the standard normal distribution at the specified confidence level, \\({\\sigma}\\) is the population standard deviation, and \\(n\\) is the sample size.\nThe reasoning behind why we can use the standard normal distribution when the sample is greater than 30 even if the population standard deviation is unknown is found in the Central Limit Theorem, which says that as the sample size increases, the sampling distribution of the sample mean approaches a normal distribution. This means that when the sample size is greater than 30 we can use the sample standard deviation to estimate the population standard deviation and create a confidence interval as seen below\n\\(CI = \\bar{X} \\pm Z_{\\alpha/2} \\times \\frac{S}{\\sqrt{n}}\\)\nOnce again the critical value for the z-distribution is the value of \\(Z_{\\alpha/2}\\) such that the area under the standard normal distribution curve between \\(-Z_{\\alpha/2}\\) and \\(Z_{\\alpha/2}\\) is equal to the confidence level.\nThe critical values for the z-distribution can be found using a z-table such as the one below. All values in the table are the area under the curve to the left of the z-score.\n\n\n\n\n\n\n\nExercise 2: z-distribution confidence intervals\n\n\n\nThe table below shows summary data for the first round of the 2023 Masters Tournament for PGA professional golfers. The critical value is for a 95% confidence interval.\n\n\n\n\n\n\n\n\n\n\n\n\nSample\nSample Mean\nStandard Deviation Estimate\nSample Size\nCritical Value\n\n\n\n\nPGA Round 1\n71.54545\n3.023477\n55\n1.959964\n\n\n\n\n\nUse the formula for creating a confidence interval using the standard normal distribution to calculate the upper and lower limits of a confidence interval for the true mean of PGA professional scoring at Augusta using the PGA pros in the first round as our sample. Use a 95% confidence interval (\\(\\alpha = .05\\)).\n\nWhy can we use the standard normal distribution to calculate this confidence interval?\nWhat is the confidence interval for the true mean of scoring for PGA professionals at Augusta National?\nWhat is the interpretation of this confidence interval?",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scoring Average Confidence Intervals (No R)"
    ]
  },
  {
    "objectID": "golf/pga_masters_nor/index.html#test-statistics",
    "href": "golf/pga_masters_nor/index.html#test-statistics",
    "title": "PGA - Scoring Average Confidence Intervals (No R)",
    "section": "Test Statistics",
    "text": "Test Statistics\nLike confidence intervals, we have two different tests for hypothesis testing for the population mean. Remember that if the population standard deviation is unknown and the sample size is less than 30, we use the t-distribution. If the population standard deviation is known or the sample size is greater than 30, we use the standard normal distribution.\nEach of these distributions have their own tests, the t-test and the z-test. This means that we have different test statistics to calculate depending on the situation.\n\nt-test\nThe t-test statistic is calculated using the formula:\n\\[t = \\frac{\\bar{x} - \\mu_0}{\\frac{s}{\\sqrt{n}}}\\]\nwhere \\(\\bar{x}\\) is the sample mean, \\(\\mu_0\\) is the hypothesized population mean, \\(s\\) is the sample standard deviation, and \\(n\\) is the sample size.\n\n\nz-test\nThe z-test statistic is calculated using the formula:\n\\[z = \\frac{\\bar{x} - \\mu_0}{\\frac{\\sigma}{\\sqrt{n}}}\\]\nwhere \\(\\bar{x}\\) is the sample mean, \\(\\mu_0\\) is the hypothesized population mean, \\(\\sigma\\) is the population standard deviation, and \\(n\\) is the sample size.",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scoring Average Confidence Intervals (No R)"
    ]
  },
  {
    "objectID": "golf/pga_masters_nor/index.html#to-reject-or-fail-to-reject",
    "href": "golf/pga_masters_nor/index.html#to-reject-or-fail-to-reject",
    "title": "PGA - Scoring Average Confidence Intervals (No R)",
    "section": "To Reject or Fail to Reject",
    "text": "To Reject or Fail to Reject\nThere are two ways to make a decision about the null hypothesis.\nMethod 1: Critical values, along with test statistics, can be used to determine if the hypothesized population mean is within the confidence interval for the true mean.\nA critical value is a value that separates the rejection region from the non-rejection region. The rejection region is the area where the null hypothesis is rejected. The non-rejection region is the area where the null hypothesis is not rejected. The critical value is determined by the significance level (\\(\\alpha\\)) and the degrees of freedom (if it is a t-test). The critical value is compared to the test statistic to determine if the null hypothesis should be rejected. If the test statistic is within the non-rejection region, the null hypothesis is not rejected. If the test statistic is within the rejection region, the null hypothesis is rejected and the alternative hypothesis is accepted.\nBelow is an example of using critical values and a test-statistic for a z-test with a 95% confidence level (two-sided). The critical value is 1.96. This means that if the test statistic is greater than 1.96 or less than -1.96, the null hypothesis is rejected. The blue represents the non-rejection region and the red the rejection region. Since the Test Statistic for this hypothetical example is 1.1 (less than 1.96 and greater than -1.96), we fail to reject the null hypothesis.\n\n\n\n\n\n\n\n\n\nThis method corresponds directly to the related confidence intervals produced for the sample data.\nIf the hypothesized population mean is within the confidence interval, we fail to reject the null hypothesis. If the hypothesized population mean is not within the confidence interval, the null hypothesis is rejected and the alternative hypothesis is accepted.\n\n\nNote: We can say that there is significant evidence to accept the alternative hypothesis if the null hypothesis is rejected. However, it should never be said that we accept the null hypothesis. We can only fail to reject it.\nMethod 2: The second method is to use a p-value. The p-value is the probability of observing a test statistic as extreme as the one calculated from the sample data given that the null hypothesis is true. The p-value is compared to the significance level (\\(\\alpha\\)) to determine if the null hypothesis should be rejected. If the p-value is less than \\(\\alpha\\), the null hypothesis is rejected. If the p-value is greater than \\(\\alpha\\), the null hypothesis is not rejected.\n\n\nNOTE: Our alternative hypothesis determines whether we are looking for the probability that the test statistic is greater than or less than the observed value.\n\nFor a two-sided test, the p-value is the probability that the test statistic is greater than the observed value or less than the negative of the observed value. Find the area in one of the tails and double it.\nFor a left-tailed test (\\(H_a: \\mu &lt; \\mu_0\\)), the p-value is the probability that the test statistic is less than the observed value.\nFor a right-tailed test (\\(H_a: \\mu &gt; \\mu_0\\)), the p-value is the probability that the test statistic is greater than the observed value.\n\nFor example, if the p-value for a single mean hypothesis test is 0.03 and the significance level is 0.05, the null hypothesis is rejected because the p-value is less than the significance level.",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scoring Average Confidence Intervals (No R)"
    ]
  },
  {
    "objectID": "golf/pga_masters_nor/index.html#hypothesis-testing-example",
    "href": "golf/pga_masters_nor/index.html#hypothesis-testing-example",
    "title": "PGA - Scoring Average Confidence Intervals (No R)",
    "section": "Hypothesis testing example",
    "text": "Hypothesis testing example\nSuppose we have a sample of data with the following values: 10, 20, 30, 40, 50, 60, 70, 80, 90, 100. We want to test if the true mean of the population is not equal to 50. Alpha is 0.05. Our hypotheses are as follows:\n\nNull Hypothesis (\\(H_0\\)): \\(\\mu = 50\\)\nAlternative Hypothesis (\\(H_1\\)): \\(\\mu \\neq 50\\)\n\nWe can use the t-test to test this hypothesis. We start by finding the sample mean and sample standard deviation. The sample mean is 55 and the sample standard deviation is 30.2765. The degrees of freedom are 9. The critical value for a 95% two-sided confidence t-interval with 9 degrees of freedom is \\(\\pm 2.262157\\).\nThe test statistic can be calculated as follows:\n\\[t = \\frac{\\bar{x} - \\mu_0}{\\frac{s}{\\sqrt{n}}}\\] \\[t = \\frac{55 - 50}{\\frac{30.2765}{\\sqrt{10}}}\\] \\[t = 0.522\\] Since the test statistic is between the two critical values, there is not enough evidence to reject the null hypothesis and conclude that the true mean is not equal to 50.\nThe p-value for this example could be calculated by finding the area under the curve to the left of -0.522 and to the right of 0.522. The p-value would be the sum of these two areas. By looking at the t-table, it can be seen that the sum of these two areas would be well greater than .5, which is greater than .05. Therefore, the null hypothesis would not be rejected.\n\n\nNOTE: Since this is a two-sided test you could simply find the probability that the test statistic is greater than 0.522 and multiply by 2. This would give you the p-value.",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scoring Average Confidence Intervals (No R)"
    ]
  },
  {
    "objectID": "golf/pga_masters_nor/index.html#hypothesizing-par-as-the-population-mean",
    "href": "golf/pga_masters_nor/index.html#hypothesizing-par-as-the-population-mean",
    "title": "PGA - Scoring Average Confidence Intervals (No R)",
    "section": "Hypothesizing Par as the Population Mean",
    "text": "Hypothesizing Par as the Population Mean\n\n\nAugusta National is breathtakingly beautiful, but if golfers get distracted by the scenic views, tall pines, bunkers, water, and azaleas may catch their balls.\nThe 13th hole at Augusta National \nImage Source: Golf.com\nIn golf par is considered to be the number of strokes a good golfer is expected to take. The par for the course at Augusta National is 72. It is known that Augusta National is a tougher than usual course, but we would like to test if that is the case for different groups of golfers.\nOur null hypothesis will generally be that the true mean of the group is equal to 72.\n\n\n\n\n\n\nExercise 3: t-test for single mean hypothesis testing\n\n\n\n\nAmateurs, who are not yet professional golfers, are generally expected to score higher than professionals. We would like to test if the mean of amateur scoring is above par at Augusta National\n\nWhat is the null hypothesis for this test?\nWhat is the alternative hypothesis for this test?\n\nPerform a single mean hypothesis test to see if the mean of amateur scoring is greater than 72 using the amateur round 1 score sample from earlier in the lesson. The summary of the sample is as follows:\n\n\n\n\n\nSample\nSample Mean\nSample Standard Deviation\nSample Size\n\n\n\n\nAmateurs Round 1\n71.54545\n3.21455\n7\n\n\n\n\n\n\nWhat is the critical value for this test?\nWhat is the test statistic?\nWhat is the p-value?\nBased on the p-value and the test statistic to critical value comparison, is there statistically significant evidence that the mean of amateur scoring at Augusta National is greater than 72?\n\n\n\n\nAmateurs generally struggle in the Masters, but in 2023 Sam Bennett, a Texas A&M student, made the cut and finished 16th. However, due to his amateur status, he was not eligible to win money and missed out on $261,000.\nSam Bennett \nImage Source: Sportico\n\n\n\n\n\n\nExercise 4: z-test for single mean hypothesis testing\n\n\n\nPGA professionals would generally average somewhere around par. We would like to test if the mean of PGA professional scoring is not equal to 72 at Augusta National.\n\nWhat is the null hypothesis for this test?\nWhat is the alternative hypothesis for this test?\n\nPerform a single mean hypothesis test using the PGA Round 1 sample from earlier. The summary of that sample is displayed below.\n\n\n\n\n\nSample\nSample Mean\nSample Standard Deviation\nSample Size\n\n\n\n\nPGA Round 1\n71.54545\n3.21455\n7\n\n\n\n\n\n\nWhat is the critical value for this test?\nWhat is the test statistic?\nWhat is the p-value?\nWhat is your conclusion?\nExplain your answer to part d.",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scoring Average Confidence Intervals (No R)"
    ]
  },
  {
    "objectID": "golf/pga_gir/index.html#using-prop.test-to-calculate-confidence-intervals",
    "href": "golf/pga_gir/index.html#using-prop.test-to-calculate-confidence-intervals",
    "title": "PGA - Scheffler Greens in Regulation",
    "section": "Using prop.test to Calculate Confidence Intervals",
    "text": "Using prop.test to Calculate Confidence Intervals\nIn R the prop.test() function can be used to calculate confidence intervals for proportions. The prop.test() function takes the number of successes and the total number of trials as arguments and returns a confidence interval for the proportion.\n\n\nTIP: You can type ?prop.test in the R console to get more information about the prop.test() function, including which arguments it takes and what it returns.\nTIP: Throughout this lesson, we will set the correct argument to FALSE in the prop.test() function. This is because the correct argument is used to apply a continuity correction to the confidence interval, which we will not be using in this lesson.\nBelow is an example of how to use the prop.test() function to calculate a 95% confidence interval for data where the number of successes is 60 and the total number of trials is 100. If $conf.int is added to the end of the prop.test() function, the function will return only the confidence interval and confidence level.\n\nprop.test(60, 100, conf.level = .95, correct = FALSE)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  60 out of 100, null probability 0.5\nX-squared = 4, df = 1, p-value = 0.0455\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5020026 0.6905987\nsample estimates:\n  p \n0.6 \n\nprop.test(60, 100, conf.level = .95, correct = FALSE)$conf.int\n\n[1] 0.5020026 0.6905987\nattr(,\"conf.level\")\n[1] 0.95\n\n\n\n\nTIP: The $ operator is used to access a specific element of a list in R. In this case, the conf.int element of the list returned by the prop.test() function contains the confidence interval.\n\n\n\n\n\n\nExercise 4: Using R Functions for Confidence Intervals\n\n\n\nUse the prop.test() function to calculate a 95% confidence interval for the proportion of greens hit in regulation by Scottie Scheffler from the bunker.\n\nWhat is the value of the lower limit of the confidence interval for the bunker cut?\nWhat is the value of the upper limit of the confidence interval for the bunker cut?\nIs the range of the confidence interval for the bunker cut wider or narrower than the range of the confidence interval for the fairway cut?",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scheffler Greens in Regulation"
    ]
  },
  {
    "objectID": "golf/pga_gir_nor/index.html",
    "href": "golf/pga_gir_nor/index.html",
    "title": "PGA - Scheffler Greens in Regulation (No R)",
    "section": "",
    "text": "Facilitation notes\n\n\n\n\n\n\nThis module would be suitable for an in-class lab or take-home assignment in an introductory statistics course.\nThe data used for this lab and an answer template can be downloaded below.\n\ndata\nStudent Quarto template",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scheffler Greens in Regulation (No R)"
    ]
  },
  {
    "objectID": "golf/pga_gir_nor/index.html#terms-to-know",
    "href": "golf/pga_gir_nor/index.html#terms-to-know",
    "title": "PGA - Scheffler Greens in Regulation (No R)",
    "section": "Terms to know",
    "text": "Terms to know\nBefore proceeding with the analysis, let’s make sure we know some golf terminology that will help us putt-putt our way through this lab.\n\n\nAre greens in regulation important to scoring? Check out the table below to see how greens in regulation and lower handicaps go hand and hand  Image source: The Range by The Grint\n\nA par 3 hole should take 1 shot to reach the green in regulation \nA par 4 hole should take 2 shots to reach the green in regulation \nA par 5 hole should take 3 shots to reach the green in regulation  Images source: Tanglewood Golf Course\n\n\nLie Terminology\n\nThe fairway is the short grass between the tee box and the green, where the ball is supposed to be hit on a par 4 or par 5 hole\nA bunker is a hazard filled with sand\n\nA fairway bunker is a bunker located in or next to the fairway\n\n\n\n\n\n\n\n\nPars and Greens in Regulation\n\n\n\n\nA par in golf is the number of strokes a good golfer is expected to take on a hole or course\nA green in regulation (GIR) is when a golfer reaches the green in the expected number of strokes (or fewer) on a par 3, 4, or 5 hole\n\nFor example, on a par 4 hole, a golfer would be expected to reach the green in 2 strokes (a drive and an approach shot) and then putt out in 2 more strokes for a total of 4 strokes. If a golfer reaches the green in 2 strokes, they have hit the green in regulation.\nOn a par 3 hole, a golfer would be expected to reach the green in 1 stroke\nOn a par 5 hole, a golfer would be expected to reach the green in 3 strokes.",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scheffler Greens in Regulation (No R)"
    ]
  },
  {
    "objectID": "golf/pga_gir_nor/index.html#making-confidence-intervals",
    "href": "golf/pga_gir_nor/index.html#making-confidence-intervals",
    "title": "PGA - Scheffler Greens in Regulation (No R)",
    "section": "Making Confidence Intervals",
    "text": "Making Confidence Intervals\nThe statistical notation for a confidence interval for a proportion is:\n\\[\n\\hat{p} \\pm z \\times \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n\\]\nWhere:\n\n\\(\\hat{p}\\) is the sample proportion.\n\\(z\\) is the z-score that corresponds to the desired level of confidence\n\\(n\\) is the sample size\n\n\n\nTIP: The z-score for a 95% confidence interval is approximately 1.96. You can use this value to calculate the confidence interval for the proportion of greens hit in regulation by Scottie Scheffler from the fairway.\n\nSome common Confidence Intervals (two-sided) and their corresponding z-scores are:\n\n\n\nConfidence Interval\nZ-Score\n\n\n\n\n90 %\n1.65\n\n\n95 %\n1.96\n\n\n98 %\n2.33\n\n\n99 %\n2.58\n\n\n\n\nFor a quick example if the proportion of success for a problem is .6, the sample size is 100, and the test is being performed at the 95% confidence level, then the confidence interval for the proportion of success can be calculated as follows:\n\\[\n\\begin{align*}\nCI &= \\hat{p} \\pm z \\times \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\\\\nCI &= .6 \\pm 1.96 \\times \\sqrt{\\frac{.6(1-.6)}{100}} \\\\\nCI &= (.50398, .696102)\n\\end{align*}\n\\]\nWe are 95% confident that the true proportion of success is between .50398 and .696102.\n\n\n\n\n\n\nExercise 2: Probability Confidence Intervals\n\n\n\nUse the numbers from the data below to calculate a 95% confidence interval for the proportion of greens hit in regulation by Scottie Scheffler from the fairway.\n\n\n\n\n\nplayer\nyear\ngreens\nholes\ncut\n\n\n\n\nScottie Scheffler\n2023\n657\n760\nFairway\n\n\n\n\n\n\nWhat is value of \\(\\hat{p}\\) for the fairway cut?\nWhat is the value of \\(n\\) for the fairway cut?\nWhat is the estimated value of the upper limit of the confidence interval from the fairway cut?\nWhat is the estimated value of the lower limit of the confidence interval from the fairway cut?\n\n\n\n\n\n\n\n\n\nExercise 3: Probability Confidence Intervals- Part 2\n\n\n\nCalculate a 95% confidence interval for the proportion of greens hit in regulation by Scottie Scheffler from the bunker.\n\n\n\n\n\nplayer\nyear\ngreens\nholes\ncut\n\n\n\n\nScottie Scheffler\n2023\n28\n48\nBunker\n\n\n\n\n\n\nWhat is the value of the lower limit of the confidence interval from the bunker cut?\nWhat is the value of the upper limit of the confidence interval from the bunker cut?\nIs the range of the confidence interval from the bunker cut wider or narrower than the range of the confidence interval from the fairway cut?\nExplain your answer to part c.",
    "crumbs": [
      "Home",
      "Golf",
      "PGA - Scheffler Greens in Regulation (No R)"
    ]
  },
  {
    "objectID": "baseball/mlb_prime_age/index.html",
    "href": "baseball/mlb_prime_age/index.html",
    "title": "What’s the prime age of an MLB player?",
    "section": "",
    "text": "Facilitation notes\n\n\n\n\n\n\nThis module would be suitable for an in-class lab or take-home assignment in an introductory data science course that uses R.\nIt assumes a basic familiarity with the RStudio Environment and basic introduction to the tidyverse has already been covered, but tips on tidyverse code are provided throughout.\nStudents should be provided with the following data files (.rds) and Quarto document (.qmd) to produce visualizations and write up their answers to each exercise. Their final deliverable is to turn in an .html document produced by “Rendering” the .qmd.\n\nbatter data\npitcher data\nStudent Quarto template\n\nPosit Cloud (via an Instructor account) or Github classroom are good options for disseminating files to students, but simply uploading files to your university’s course management system works, too.",
    "crumbs": [
      "Home",
      "Baseball",
      "What's the prime age of an MLB player?"
    ]
  },
  {
    "objectID": "baseball/mlb_prime_age/index.html#data",
    "href": "baseball/mlb_prime_age/index.html#data",
    "title": "What’s the prime age of an MLB player?",
    "section": "Data",
    "text": "Data\nThis data was curated via the baseballr package, which has built in functions for acquiring a plethora of baseball data. In particular, data for this module was pulled in May 2024 using the fg_batter_leaders() and fg_pitcher_leaders() functions, which provide over 300 variables of statistics on batters and pitchers beginning with the 1974 season. This analysis engages with only a small subset of available variables and seasons, but we encourage baseball enthusiasts and aspiring analysts to check out all the package has to offer.\n\n\nVariable Descriptions (all players) \n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nx_mlbamid\nunique MLB player id\n\n\nseason\nseason the row of data comes from\n\n\nteam_name\nthree-character MLB team abbreviation\n\n\nplayer_name\nfirst and last name of MLB player\n\n\nage\nage of MLB during the corresponding season\n\n\nwar\nWins Above Replacement - an estimation of the amount of wins the player would bring to the team, above a regular replacement level player. It measures the value of a player and how much better they are than an “average” one.\n\n\ng\nnumber of games the player played during the corresponding season\n\n\n\n\n\nVariable Descriptions (batters only) \n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nbats\nwhether player bats right-handed or left-handed\n\n\nab\nnumber of at bats the player had during the corresponding season\n\n\npa\nnumber of plate appearances the player had during the corresponding season\n\n\nh\nnumber of hits the player had during the corresponding season\n\n\nx1b\nnumber of singles the player hit during the corresponding season\n\n\nx2b\nnumber of doubles the player hit during the corresponding season\n\n\nx3b\nnumber of triples the player hit during the corresponding season\n\n\nhr\nnumber of home runs the player hit during the corresponding season\n\n\nr\nnumber of runs the player had during the corresponding season\n\n\nrbi\nnumber of runs batted in the player had during the corresponding season\n\n\n\n\n\nVariable Descriptions (pitchers only) \n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nthrows\nnumber of runs batted in the player had during the corresponding season\n\n\nip\nnumber of innings the player pitched during the corresponding season\n\n\nera\nEarned Run Average: average number of earned runs given up by a pitcher per nine innings pitched during the corresponding season\n\n\nwhip\nWalks plus Hits per Inning Pitched. A measure of the number of base-runners a pitcher has allowed per inning pitched during the corresponding season.\n\n\npitches\ntotal number of pitches thrown by the pitcher during the corresponding season.\n\n\nballs\ntotal number of pitches thrown by the pitcher that were called as balls during the corresponding season.\n\n\nstrikes\ntotal number of pitches thrown by the pitcher that were called as strikes during the corresponding season.",
    "crumbs": [
      "Home",
      "Baseball",
      "What's the prime age of an MLB player?"
    ]
  },
  {
    "objectID": "baseball/mlb_prime_age/index.html#batters",
    "href": "baseball/mlb_prime_age/index.html#batters",
    "title": "What’s the prime age of an MLB player?",
    "section": "Batters",
    "text": "Batters\n\n\n\n\n\n\nResearch question\n\n\n\nWhat is the average age a batter in the MLB reaches his prime?\n\n\nLet’s first note how our data is organized:\n\nhead(batter_stats)\n\n# A tibble: 6 × 18\n  x_mlbamid season team_name bats  player_name       age   war     g    ab    pa\n      &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    545361   2014 LAA       R     Mike Trout         22  8.29   157   602   705\n2    457763   2014 SFG       R     Buster Posey       27  7.52   147   547   605\n3    518960   2014 MIL       R     Jonathan Lucroy    28  7.44   153   585   655\n4    457705   2014 PIT       R     Andrew McCutch…    27  7.40   146   548   648\n5    519317   2014 MIA       R     Giancarlo Stan…    24  6.85   145   539   638\n6    488726   2014 CLE       L     Michael Brantl…    27  6.53   156   611   676\n# ℹ 8 more variables: h &lt;dbl&gt;, x1b &lt;dbl&gt;, x2b &lt;dbl&gt;, x3b &lt;dbl&gt;, hr &lt;dbl&gt;,\n#   r &lt;dbl&gt;, rbi &lt;dbl&gt;, best_war &lt;dbl&gt;\n\n\n\n\n\nCODE TIP: The function head() returns the first 6 rows of a dataset, and the function tail() returns the last 6. You can add the argument n = to display a different number of rows. Note these are base R functions and do not require the tidyverse to use.\nIf we arrange by x_mlbaid we can see that there can be multiple observations per player, where each row represents a different season.\n\nbatter_stats |&gt; \n  arrange(x_mlbamid) |&gt; \n  slice(1:10)\n\n# A tibble: 10 × 18\n   x_mlbamid season team_name bats  player_name    age     war     g    ab    pa\n       &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1    110029   2014 NYM       L     Bobby Abreu     40 -0.213     78   133   155\n 2    112526   2014 NYM       R     Bartolo Col…    41 -0.588     31    62    69\n 3    112526   2015 NYM       R     Bartolo Col…    42 -0.0408    33    58    64\n 4    112526   2016 NYM       R     Bartolo Col…    43 -0.243     34    60    65\n 5    112526   2017 - - -     R     Bartolo Col…    44 -0.266     28    19    20\n 6    112526   2018 TEX       R     Bartolo Col…    45 -0.0475    28     4     4\n 7    114739   2014 CLE       L     Jason Giambi    43 -0.496     26    60    70\n 8    115629   2014 COL       R     LaTroy Hawk…    41 -0.0141    57     1     1\n 9    115629   2015 - - -     R     LaTroy Hawk…    42 -0.0146    42     1     1\n10    116338   2014 DET       R     Torii Hunter    38  1.11     142   549   586\n# ℹ 8 more variables: h &lt;dbl&gt;, x1b &lt;dbl&gt;, x2b &lt;dbl&gt;, x3b &lt;dbl&gt;, hr &lt;dbl&gt;,\n#   r &lt;dbl&gt;, rbi &lt;dbl&gt;, best_war &lt;dbl&gt;\n\n\n\n\n The pipe: Recall that |&gt; is called the “pipe” function and can be read as “and then.” In English, the code on the left can be read as “take the batter_stats data and then arrange it by x_mlbamid and then slice the first 10 rows.” Mathematically, the pipe accomplishes f(g(x)) with the (psudeo-)code x |&gt; g() |&gt; f(). Read more about the pipe here.\n\ndplyr: arrange() and slice() are examples of dplyr verbs: tidyverse functions that do something to / act on the data. Other examples include filter(), select(), mutate(), group_by(), summarize(), relocate(), and many more. These verbs are often chained together with the pipe to accomplish multiple data wrangling tasks. Read more about data wrangling with dplyr here.\n\n\n\n\n\n\n\nExercise 1:\n\n\n\nWhich seasons are included in this data?\n\n\n\n\nTIP: Try writing your answer as a full sentence in the .qmd using inline code. For example, if you have the first season saved in an object first_season, then including `r first_season` outside a code chunk will allow you to auto-populate this value in a sentence.\n\n\n\n\n\n\nImportant\n\n\n\nIn order to determine the prime age of each player, we need to look for the year in which his war reached its player-specific maximum. We can utilize the group_by() function to do this.\n\n\n\n\n\n\n\n\nExercise 2:\n\n\n\nCopy, paste the following code and fill in the blanks to create a new variable best_war that contains a player’s maximum war.\n\nbatter_stats &lt;- batter_stats |&gt; \n  group_by(________) |&gt; \n  mutate(_______ = _______(_______)) |&gt; \n  ungroup()\n\n\n\n\n\nCODE TIP: group_by() allows all subsequent actions to be done for each group of the grouping variable. Therefore, if we group by player id, we’re able to determine the maximum war for each player, not simply the maximum war for the whole dataset. It’s often a good idea to ungroup() at the end of a chain of code, otherwise the next time you try to use your data, it will still perform every operation by group.\nTake a quick glimpse() of your data to confirm the first few values of best_war match those below before proceeding.\n\n\nRows: 13,917\nColumns: 18\n$ x_mlbamid   &lt;dbl&gt; 545361, 457763, 518960, 457705, 519317, 488726, 543685, 43…\n$ season      &lt;dbl&gt; 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014…\n$ team_name   &lt;chr&gt; \"LAA\", \"SFG\", \"MIL\", \"PIT\", \"MIA\", \"CLE\", \"WSN\", \"TOR\", \"P…\n$ bats        &lt;chr&gt; \"R\", \"R\", \"R\", \"R\", \"R\", \"L\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\"…\n$ player_name &lt;chr&gt; \"Mike Trout\", \"Buster Posey\", \"Jonathan Lucroy\", \"Andrew M…\n$ age         &lt;dbl&gt; 22, 27, 28, 27, 24, 27, 24, 33, 31, 35, 28, 28, 31, 23, 30…\n$ war         &lt;dbl&gt; 8.2866, 7.5222, 7.4368, 7.4014, 6.8473, 6.5310, 6.4054, 6.…\n$ g           &lt;dbl&gt; 157, 147, 153, 146, 145, 156, 153, 155, 111, 148, 148, 158…\n$ ab          &lt;dbl&gt; 602, 547, 585, 548, 539, 611, 613, 553, 379, 549, 574, 608…\n$ pa          &lt;dbl&gt; 705, 605, 655, 648, 638, 676, 683, 673, 460, 614, 644, 695…\n$ h           &lt;dbl&gt; 173, 170, 176, 172, 155, 200, 176, 158, 110, 178, 163, 155…\n$ x1b         &lt;dbl&gt; 89, 118, 108, 103, 86, 133, 110, 96, 79, 125, 102, 93, 134…\n$ x2b         &lt;dbl&gt; 39, 28, 53, 38, 31, 45, 39, 27, 20, 33, 34, 31, 37, 37, 34…\n$ x3b         &lt;dbl&gt; 9, 2, 2, 6, 1, 2, 6, 0, 0, 1, 4, 2, 2, 9, 1, 1, 2, 1, 3, 1…\n$ hr          &lt;dbl&gt; 36, 22, 13, 25, 37, 20, 21, 35, 11, 19, 23, 29, 14, 16, 19…\n$ r           &lt;dbl&gt; 115, 72, 73, 89, 89, 94, 111, 101, 45, 79, 95, 93, 77, 92,…\n$ rbi         &lt;dbl&gt; 111, 89, 69, 83, 105, 97, 83, 103, 67, 77, 73, 98, 82, 69,…\n$ best_war    &lt;dbl&gt; 9.4559, 7.5222, 7.4368, 7.4014, 6.8473, 6.5310, 6.7801, 6.…\n\n\n\n\nCODE TIP: In real life data science work, you won’t usually be provided with the “corect” answer to compare to, so it’s often a good idea to do a quick check after any data transformation to make sure your code did what you expected. In this case, you might choose one player to verify that their best_war value is in fact equal to their maximum war value. You can do a quick filter for that player in your console, or use the search feature when Viewing the full data in spreadsheet view.\n\n\n\n\n\n\nExercise 3:\n\n\n\nCreate a new dataset called prime_age that keeps only the rows where a player’s war is equal to his best_war.\nWhat are the dimensions of this new dataset?\n\n\n\n\n\nHint: what dyplr verb do you need to keep rows that meet a criteria?\nIdeally, we want there to be one row per player in our new dataset. However, if we check the number of unique players we have in our original data, we find this does not match the number of rows in prime_age.\n\n\nCODE TIP: Two options for checking the number of unique levels of a variable are length(unique(data$variable)) or data |&gt; distinct(variable) |&gt; nrow()\n\n\n\n\n\n\nExercise 4:\n\n\n\nReport the number of unique players in the dataset batter_stats.\nInspect the prime_age data more closely. What is the maximum number of rows that appear for a player in this dataset? Comment on why this is happening. Hint: creating a new variable that counts the number of rows per id can help you investigate this.\n\n\n\n\nCODE TIP: group_by(grouping_variable) followed by mutate(n = n()) will count the number of rows per level of the grouping variable.\n\n\n\n\n\n\nExercise 5:\n\n\n\nDetermine a strategy for reducing prime_age down to one row per person (still maintaining all relevant columns). Describe your strategy in words and then write code to accomplish it. Careful - don’t just arbitrarily throw away rows! There are multiple ways you might approach this, but you should justify your decision(s) and think through implications for your ultimate analysis goal: estimating prime age.\n\n\nYour reduced prime_age should look something like this:\n\n\nRows: 3,752\nColumns: 18\n$ x_mlbamid   &lt;dbl&gt; 457763, 518960, 457705, 519317, 488726, 430832, 431145, 13…\n$ season      &lt;dbl&gt; 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014…\n$ team_name   &lt;chr&gt; \"SFG\", \"MIL\", \"PIT\", \"MIA\", \"CLE\", \"TOR\", \"PIT\", \"TEX\", \"M…\n$ bats        &lt;chr&gt; \"R\", \"R\", \"R\", \"R\", \"L\", \"R\", \"R\", \"R\", \"R\", \"R\", \"L\", \"L\"…\n$ player_name &lt;chr&gt; \"Buster Posey\", \"Jonathan Lucroy\", \"Andrew McCutchen\", \"Gi…\n$ age         &lt;dbl&gt; 27, 28, 27, 24, 27, 33, 31, 35, 28, 23, 30, 24, 27, 35, 29…\n$ war         &lt;dbl&gt; 7.5222, 7.4368, 7.4014, 6.8473, 6.5310, 6.1703, 6.1427, 5.…\n$ g           &lt;dbl&gt; 147, 153, 146, 145, 156, 155, 111, 148, 148, 148, 156, 140…\n$ ab          &lt;dbl&gt; 547, 585, 548, 539, 611, 553, 379, 549, 574, 558, 563, 524…\n$ pa          &lt;dbl&gt; 605, 655, 648, 638, 676, 673, 460, 614, 644, 640, 643, 616…\n$ h           &lt;dbl&gt; 170, 176, 172, 155, 200, 158, 110, 178, 163, 165, 150, 150…\n$ x1b         &lt;dbl&gt; 118, 108, 103, 86, 133, 96, 79, 125, 102, 103, 96, 89, 103…\n$ x2b         &lt;dbl&gt; 28, 53, 38, 31, 45, 27, 20, 33, 34, 37, 34, 28, 35, 37, 18…\n$ x3b         &lt;dbl&gt; 2, 2, 6, 1, 2, 0, 0, 1, 4, 9, 1, 1, 2, 1, 1, 3, 1, 7, 3, 2…\n$ hr          &lt;dbl&gt; 22, 13, 25, 37, 20, 35, 11, 19, 23, 16, 19, 32, 36, 16, 21…\n$ r           &lt;dbl&gt; 72, 73, 89, 89, 94, 101, 45, 79, 95, 92, 87, 89, 80, 85, 7…\n$ rbi         &lt;dbl&gt; 89, 69, 83, 105, 97, 103, 67, 77, 73, 69, 74, 78, 107, 82,…\n$ best_war    &lt;dbl&gt; 7.5222, 7.4368, 7.4014, 6.8473, 6.5310, 6.1703, 6.1427, 5.…\n\n\n\n\n\n\n\n\nExercise 6:\n\n\n\nProduce a visualization that explores the distribution of prime ages, for all players in this data.\n\n\n\n\n\n\n\n\nExercise 7:\n\n\n\nBased on the graph, “eyeball” an initial answer to the research question: at what age do professional batters tend to be at their “prime”?\n\n\n\n\n\n\n\n\nExercise 8:\n\n\n\nCalculate the mean and the median prime age for batters in this data.\n\n\n\n\n\n\n\n\nExercise 9:\n\n\n\nReproduce your graph from above but add 2 lines to the graph representing the mean and median of the distribution.\n\n\n\n\n\nTip: Add a layer called geom_vline to your ggplot code. Make sure the colors of the lines are different.",
    "crumbs": [
      "Home",
      "Baseball",
      "What's the prime age of an MLB player?"
    ]
  },
  {
    "objectID": "baseball/mlb_prime_age/index.html#pitchers",
    "href": "baseball/mlb_prime_age/index.html#pitchers",
    "title": "What’s the prime age of an MLB player?",
    "section": "Pitchers",
    "text": "Pitchers\n\n\n\n\n\n\nResearch question\n\n\n\nWhat is the average age an MLB pitcher reaches his prime?\n\n\n\n\n\n\n\n\nExercise 10\n\n\n\nCopy, paste, tweak appropriate code from previous exercises to determine the prime age of pitchers, using the pitcher_stats data.\n\n\n\n\n\nCheck: there are 2382 unique pitchers in the pitcher_stats data, so your final dataset for analysis should have that many rows.\n\n\n\n\n\n\nExercise 11\n\n\n\nWrite a paragraph summarizing your findings about the prime age of batters and pitchers from this analysis. Things to consider:\n\nAre the prime ages of batters and pitchers similar or different?\nDo all players hit their prime at about the same age, or is there a wide range?\nAre there limitations to this analysis?\nWhat additional analyses would you want to conduct to investigate prime age more fully?\nIs there any additional data you would want to explore further?",
    "crumbs": [
      "Home",
      "Baseball",
      "What's the prime age of an MLB player?"
    ]
  }
]